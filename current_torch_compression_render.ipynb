{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAiP1vewBOcw"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.functional as F\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vmp8-FvfBfJq",
        "outputId": "d4e16fc2-1835-4342-bc52-4deb3603e059"
      },
      "source": [
        "os.getcwd()\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UqsAPtBgqn"
      },
      "source": [
        "checkpoint = torch.load('checkpoint.pt')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNAdEeklBhR5",
        "outputId": "a61d514f-b924-4872-e5c3-a41079208484"
      },
      "source": [
        "for i in checkpoint.values():\r\n",
        "    print(i.size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 4, 8, 8])\n",
            "torch.Size([32])\n",
            "torch.Size([64, 32, 4, 4])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([512, 4096])\n",
            "torch.Size([512])\n",
            "torch.Size([4, 512])\n",
            "torch.Size([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWezKTkFBwe6"
      },
      "source": [
        "\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOGe75H-ByA5"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "os.environ.setdefault('PATH', '')\r\n",
        "from collections import deque\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import cv2\r\n",
        "cv2.ocl.setUseOpenCL(False)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class NoopResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, noop_max=30):\r\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\r\n",
        "        No-op is assumed to be action 0.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.noop_max = noop_max\r\n",
        "        self.override_num_noops = None\r\n",
        "        self.noop_action = 0\r\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        if self.override_num_noops is not None:\r\n",
        "            noops = self.override_num_noops\r\n",
        "        else:\r\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\r\n",
        "        assert noops > 0\r\n",
        "        obs = None\r\n",
        "        for _ in range(noops):\r\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\r\n",
        "            if done:\r\n",
        "                obs = self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class FireResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(1)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(2)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class EpisodicLifeEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\r\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.lives = 0\r\n",
        "        self.was_real_done  = True\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        obs, reward, done, info = self.env.step(action)\r\n",
        "        self.was_real_done = done\r\n",
        "        # check current lives, make loss of life terminal,\r\n",
        "        # then update lives to handle bonus lives\r\n",
        "        lives = self.env.unwrapped.ale.lives()\r\n",
        "        if lives < self.lives and lives > 0:\r\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\r\n",
        "            # so it's important to keep lives > 0, so that we only reset once\r\n",
        "            # the environment advertises done.\r\n",
        "            done = True\r\n",
        "        self.lives = lives\r\n",
        "        return obs, reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\"Reset only when lives are exhausted.\r\n",
        "        This way all states are still reachable even though lives are episodic,\r\n",
        "        and the learner need not know about any of this behind-the-scenes.\r\n",
        "        \"\"\"\r\n",
        "        if self.was_real_done:\r\n",
        "            obs = self.env.reset(**kwargs)\r\n",
        "        else:\r\n",
        "            # no-op step to advance from terminal/lost life state\r\n",
        "            obs, _, _, _ = self.env.step(0)\r\n",
        "        self.lives = self.env.unwrapped.ale.lives()\r\n",
        "        return obs\r\n",
        "\r\n",
        "class MaxAndSkipEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, skip=4):\r\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        # most recent raw observations (for max pooling across time steps)\r\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\r\n",
        "        self._skip       = skip\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\r\n",
        "        total_reward = 0.0\r\n",
        "        done = None\r\n",
        "        for i in range(self._skip):\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\r\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "        # Note that the observation on the done=True frame\r\n",
        "        # doesn't matter\r\n",
        "        max_frame = self._obs_buffer.max(axis=0)\r\n",
        "\r\n",
        "        return max_frame, total_reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        return self.env.reset(**kwargs)\r\n",
        "\r\n",
        "class ClipRewardEnv(gym.RewardWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.RewardWrapper.__init__(self, env)\r\n",
        "\r\n",
        "    def reward(self, reward):\r\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\r\n",
        "        return np.sign(reward)\r\n",
        "\r\n",
        "\r\n",
        "class WarpFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\r\n",
        "        \"\"\"\r\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\r\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\r\n",
        "        observation should be warped.\r\n",
        "        \"\"\"\r\n",
        "        super().__init__(env)\r\n",
        "        self._width = width\r\n",
        "        self._height = height\r\n",
        "        self._grayscale = grayscale\r\n",
        "        self._key = dict_space_key\r\n",
        "        if self._grayscale:\r\n",
        "            num_colors = 1\r\n",
        "        else:\r\n",
        "            num_colors = 3\r\n",
        "\r\n",
        "        new_space = gym.spaces.Box(\r\n",
        "            low=0,\r\n",
        "            high=255,\r\n",
        "            shape=(self._height, self._width, num_colors),\r\n",
        "            dtype=np.uint8,\r\n",
        "        )\r\n",
        "        if self._key is None:\r\n",
        "            original_space = self.observation_space\r\n",
        "            self.observation_space = new_space\r\n",
        "        else:\r\n",
        "            original_space = self.observation_space.spaces[self._key]\r\n",
        "            self.observation_space.spaces[self._key] = new_space\r\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\r\n",
        "\r\n",
        "    def observation(self, obs):\r\n",
        "        if self._key is None:\r\n",
        "            frame = obs\r\n",
        "        else:\r\n",
        "            frame = obs[self._key]\r\n",
        "\r\n",
        "        if self._grayscale:\r\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\r\n",
        "        frame = cv2.resize(\r\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\r\n",
        "        )\r\n",
        "        if self._grayscale:\r\n",
        "            frame = np.expand_dims(frame, -1)\r\n",
        "\r\n",
        "        if self._key is None:\r\n",
        "            obs = frame\r\n",
        "        else:\r\n",
        "            obs = obs.copy()\r\n",
        "            obs[self._key] = frame\r\n",
        "        return obs\r\n",
        "\r\n",
        "\r\n",
        "class FrameStack(gym.Wrapper):\r\n",
        "    def __init__(self, env, k):\r\n",
        "        \"\"\"Stack k last frames.\r\n",
        "        Returns lazy array, which is much more memory efficient.\r\n",
        "        See Also\r\n",
        "        --------\r\n",
        "        baselines.common.atari_wrappers.LazyFrames\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.k = k\r\n",
        "        self.frames = deque([], maxlen=k)\r\n",
        "        shp = env.observation_space.shape\r\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        ob = self.env.reset()\r\n",
        "        for _ in range(self.k):\r\n",
        "            self.frames.append(ob)\r\n",
        "        return self._get_ob()\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        ob, reward, done, info = self.env.step(action)\r\n",
        "        self.frames.append(ob)\r\n",
        "        return self._get_ob(), reward, done, info\r\n",
        "\r\n",
        "    def _get_ob(self):\r\n",
        "        assert len(self.frames) == self.k\r\n",
        "        return LazyFrames(list(self.frames))\r\n",
        "\r\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.ObservationWrapper.__init__(self, env)\r\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        # careful! This undoes the memory optimization, use\r\n",
        "        # with smaller replay buffers only.\r\n",
        "        return np.array(observation).astype(np.float32) / 255.0\r\n",
        "\r\n",
        "class LazyFrames(object):\r\n",
        "    def __init__(self, frames):\r\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\r\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\r\n",
        "        buffers.\r\n",
        "        This object should only be converted to numpy array before being passed to the model.\r\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\r\n",
        "        self._frames = frames\r\n",
        "        self._out = None\r\n",
        "\r\n",
        "    def _force(self):\r\n",
        "        if self._out is None:\r\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\r\n",
        "            self._frames = None\r\n",
        "        return self._out\r\n",
        "\r\n",
        "    def __array__(self, dtype=None):\r\n",
        "        out = self._force()\r\n",
        "        if dtype is not None:\r\n",
        "            out = out.astype(dtype)\r\n",
        "        return out\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self._force())\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        return self._force()[i]\r\n",
        "\r\n",
        "    def count(self):\r\n",
        "        frames = self._force()\r\n",
        "        return frames.shape[frames.ndim - 1]\r\n",
        "\r\n",
        "    def frame(self, i):\r\n",
        "        return self._force()[..., i]\r\n",
        "\r\n",
        "def make_atari(env_id, max_episode_steps=None):\r\n",
        "    env = gym.make(env_id)\r\n",
        "    assert 'NoFrameskip' in env.spec.id\r\n",
        "    env = NoopResetEnv(env, noop_max=30)\r\n",
        "    env = MaxAndSkipEnv(env, skip=4)\r\n",
        "    if max_episode_steps is not None:\r\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\r\n",
        "    return env\r\n",
        "\r\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\r\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\r\n",
        "    \"\"\"\r\n",
        "    if episode_life:\r\n",
        "        env = EpisodicLifeEnv(env)\r\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\r\n",
        "        env = FireResetEnv(env)\r\n",
        "    env = WarpFrame(env)\r\n",
        "    if scale:\r\n",
        "        env = ScaledFloatFrame(env)\r\n",
        "    if clip_rewards:\r\n",
        "        env = ClipRewardEnv(env)\r\n",
        "    if frame_stack:\r\n",
        "        env = FrameStack(env, 4)\r\n",
        "    return env"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxKaVFOMB3BG"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdRL6hh-B8s4"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG7wgB2NB_MH",
        "outputId": "596a5fc2-70b4-406c-9cdd-5b57fa58559c"
      },
      "source": [
        "!apt-get install python-opengl -y\r\n",
        "\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "\r\n",
        "!pip install piglet\r\n",
        "\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "Display().start()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Collecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f29c9c0ee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQudhh3QCA4d"
      },
      "source": [
        "import gym\r\n",
        "from IPython import display\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBdIvmZwCDxn"
      },
      "source": [
        "# Use the Baseline Atari environment because of Deepmind helper functions\r\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\r\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\r\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\r\n",
        "#env.seed(seed)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nP9YSJOzCLyE",
        "outputId": "8eddc15d-4eb7-4ac4-e771-d062dad2b011"
      },
      "source": [
        "env.reset()\r\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\r\n",
        "# for _ in range(40):\r\n",
        "#     img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "#     display.display(plt.gcf())\r\n",
        "#     display.clear_output(wait=True)\r\n",
        "#     action = env.action_space.sample()\r\n",
        "#     env.step(action)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz8hHHkoCcKT"
      },
      "source": [
        "def compute_conv_dim(dim_size, kernel_size, padding, stride):\r\n",
        "    return int((dim_size - kernel_size + 2 * padding) / stride) + 1\r\n"
      ],
      "execution_count": 611,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu9aVLZ1COE2"
      },
      "source": [
        "valid_actions = [0,1,2,3]\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self, in_channels, valid_actions):\r\n",
        "        \"\"\"\r\n",
        "        Value estimator for DQN\r\n",
        "        :param in_channels: number of in channels for Conv2d\r\n",
        "        :param valid_actions: all valid actions\r\n",
        "        \"\"\"\r\n",
        "        super(Net, self).__init__()\r\n",
        "        # input shape batch_size x in_channels x 84 x 84\r\n",
        "        self.in_channels = in_channels\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=2)\r\n",
        "        self.r1 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\r\n",
        "        self.r2 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3,stride = 2)\r\n",
        "        self.r3 = nn.ReLU()\r\n",
        "\r\n",
        "        self.dense = torch.nn.Linear(64 * 8 * 8, 512)\r\n",
        "        self.r4 = nn.ReLU()\r\n",
        "        self.out = torch.nn.Linear(512, len(valid_actions))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        Calculates probability of each action\r\n",
        "        NOTE: a single discrete state is collection of 4 frames\r\n",
        "        :param x: processed state of shape b x in_channel x 84 x 84\r\n",
        "        :returns tensor of shape [batch_size, NUM_VALID_ACTIONS] (estimated action values)\r\n",
        "        \"\"\"\r\n",
        "        x = self.r1(self.conv1(x))  # b x 32 x 21 x 21\r\n",
        "\r\n",
        "        x = self.r2(self.conv2(x))  # b x 64 x 12 x 12\r\n",
        "\r\n",
        "        x = self.r3(self.conv3(x))\r\n",
        "\r\n",
        "\r\n",
        "        x = x.view(x.size(0), -1)  # b x (64 * 12 * 12)\r\n",
        "        dense_out = self.dense(x)  # b x 512\r\n",
        "        dense_out = self.r4(dense_out)\r\n",
        "        output = self.out(dense_out)  # b x VALID_ACTIONS\r\n",
        "        # gather valid action values for each batch based on a.\r\n",
        "        return output\r\n",
        "\r\n",
        "model = Net(in_channels = 4,valid_actions = valid_actions)"
      ],
      "execution_count": 612,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iDt1DIICV4K",
        "outputId": "5cf51d9d-4e3f-40ed-cd75-ba9d3cd544f1"
      },
      "source": [
        "model.load_state_dict(checkpoint)"
      ],
      "execution_count": 613,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 613
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl9fsIfWM_hw",
        "outputId": "f794dc6c-efdf-4946-ea4a-1fa128dc1399"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 614,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
            "  (r1): ReLU()\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "  (r2): ReLU()\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (r3): ReLU()\n",
            "  (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
            "  (r4): ReLU()\n",
            "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkeXRMa0DiCn"
      },
      "source": [
        "state = np.array(env.reset())"
      ],
      "execution_count": 615,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d8WedhDtj0"
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 616,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgx0OC37Duzv"
      },
      "source": [
        "\r\n",
        "x=np.moveaxis(state, -1, 0)"
      ],
      "execution_count": 617,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1kXaS9ADwBZ"
      },
      "source": [
        "x=x.reshape(1,4,84,84)"
      ],
      "execution_count": 618,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZTjRCssDxK5",
        "outputId": "08ee67ac-22a2-4524-d0c0-d078591d3323"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "X_batch = Variable(torch.from_numpy(x))\r\n",
        "\r\n",
        "X_batch=X_batch.float()\r\n",
        "model(X_batch)"
      ],
      "execution_count": 619,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0423, 0.0766, 0.0211, 0.0101]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 619
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEV0DY5cX4Rf",
        "outputId": "57d98f2d-16bb-4bc6-a422-42671a17ddff"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 620,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
            "  (r1): ReLU()\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "  (r2): ReLU()\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (r3): ReLU()\n",
            "  (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
            "  (r4): ReLU()\n",
            "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aIt9kbaXgaE"
      },
      "source": [
        "import copy\r\n",
        "\r\n",
        "class CompNet(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, network, r,verbose = False):\r\n",
        "        super(CompNet, self).__init__()\r\n",
        "        \r\n",
        "        self.network = network                 # network is an instance of Net class. Copying it not to change the initial network.\r\n",
        "        self.r = r                             # r is an int between 1 and 1000 (due to layers are described by 1000 by 1000 matrixes)\r\n",
        "        self.verbose = verbose\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=self.network.in_channels, out_channels=32, kernel_size=8, stride=2)\r\n",
        "        self.r1 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\r\n",
        "        self.r2 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3,stride = 2)\r\n",
        "        self.r3 = nn.ReLU()\r\n",
        "\r\n",
        "        self.fc1 = self.compress_(copy.deepcopy(self.network.dense))\r\n",
        " \r\n",
        "        self.fc2 = copy.deepcopy(self.network.out)\r\n",
        "        self.ReLU = nn.ReLU()\r\n",
        "        \r\n",
        "                                                     \r\n",
        "\r\n",
        "        \r\n",
        "    def compress_(self,layer):            # a function,used to compress a layer of the Net, e.g. network.fc1\r\n",
        "        \r\n",
        "        verbose = self.verbose\r\n",
        "        for pars in layer.parameters():\r\n",
        "            if pars.size() == (512,4096):\r\n",
        "                u,s,v = torch.svd(pars,compute_uv = True)\r\n",
        "                \r\n",
        "                u = u[:,:self.r]\r\n",
        "                s = s[:self.r]\r\n",
        "                v = v[:,:self.r]\r\n",
        "                \r\n",
        "                \r\n",
        "                new_layer_v = torch.nn.Linear(512,self.r, bias = False)\r\n",
        "                new_layer_v.weight = torch.nn.parameter.Parameter(v.t())\r\n",
        "                \r\n",
        "                new_layer_s = torch.nn.Linear(self.r,self.r, bias = False)\r\n",
        "                new_layer_s.weight = torch.nn.parameter.Parameter(torch.diag(s))\r\n",
        "                \r\n",
        "                new_layer_u = torch.nn.Linear(self.r,4096, bias = False)\r\n",
        "                new_layer_u.weight = torch.nn.parameter.Parameter(u)\r\n",
        "                \r\n",
        "        if verbose:\r\n",
        "            print('Compression of the {} layer {} done.'.format(self.counter,str(layer)))\r\n",
        "        \r\n",
        "        return torch.nn.Sequential(new_layer_v,new_layer_s,new_layer_u)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        Calculates probability of each action\r\n",
        "        NOTE: a single discrete state is collection of 4 frames\r\n",
        "        :param x: processed state of shape b x in_channel x 84 x 84\r\n",
        "        :returns tensor of shape [batch_size, NUM_VALID_ACTIONS] (estimated action values)\r\n",
        "        \"\"\"\r\n",
        "        x = self.r1(self.conv1(x))  # b x 32 x 21 x 21\r\n",
        "\r\n",
        "        x = self.r2(self.conv2(x))  # b x 64 x 12 x 12\r\n",
        "\r\n",
        "        x = self.r3(self.conv3(x))\r\n",
        "\r\n",
        "\r\n",
        "        x = x.view(x.size(0), -1)  # b x (64 * 12 * 12)\r\n",
        "        dense_out = self.fc1(x)  # b x 512\r\n",
        "        output = self.fc2(dense_out)  # b x VALID_ACTIONS\r\n",
        "        # gather valid action values for each batch based on a.\r\n",
        "        return output\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Initialization\r\n",
        "\r\n",
        "comp_model_torch = CompNet(network = model,r = 5)"
      ],
      "execution_count": 647,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCgulvDkYnqB",
        "outputId": "b7a77149-bcf4-4193-86de-b4029f99cd79"
      },
      "source": [
        "print(comp_model_torch)"
      ],
      "execution_count": 648,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CompNet(\n",
            "  (network): Net(\n",
            "    (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
            "    (r1): ReLU()\n",
            "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (r2): ReLU()\n",
            "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (r3): ReLU()\n",
            "    (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
            "    (r4): ReLU()\n",
            "    (out): Linear(in_features=512, out_features=4, bias=True)\n",
            "  )\n",
            "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
            "  (r1): ReLU()\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "  (r2): ReLU()\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (r3): ReLU()\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=5, bias=False)\n",
            "    (1): Linear(in_features=5, out_features=5, bias=False)\n",
            "    (2): Linear(in_features=5, out_features=4096, bias=False)\n",
            "  )\n",
            "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
            "  (ReLU): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlzXihpnEJ5v",
        "outputId": "5f29c9a9-17a1-4769-e7b2-65348075ae37"
      },
      "source": [
        "\r\n",
        "# The first model makes the predictions for Q-values which are used to\r\n",
        "# make a action.\r\n",
        "model1 = comp_model_torch\r\n",
        "# Build a target model for the prediction of future rewards.\r\n",
        "# The weights of a target model get updated every 10000 steps thus when the\r\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\r\n",
        "model_target = CompNet(network = model,r = 5)\r\n",
        "device=torch.device('cuda:0')\r\n",
        "model1.to(device)\r\n",
        "model_target.to(device)"
      ],
      "execution_count": 649,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompNet(\n",
              "  (network): Net(\n",
              "    (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
              "    (r1): ReLU()\n",
              "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (r2): ReLU()\n",
              "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (r3): ReLU()\n",
              "    (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
              "    (r4): ReLU()\n",
              "    (out): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
              "  (r1): ReLU()\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (r2): ReLU()\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (r3): ReLU()\n",
              "  (fc1): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=5, bias=False)\n",
              "    (1): Linear(in_features=5, out_features=5, bias=False)\n",
              "    (2): Linear(in_features=5, out_features=4096, bias=False)\n",
              "  )\n",
              "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
              "  (ReLU): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 649
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US5zrzo1UyNh"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.nn import functional"
      ],
      "execution_count": 650,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGH0vekJVM_H"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "# Configuration paramaters for the whole setup\r\n",
        "seed = 42\r\n",
        "gamma = 0.99  # Discount factor for past rewards\r\n",
        "epsilon = 1.0  # Epsilon greedy parameter\r\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\r\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\r\n",
        "epsilon_interval = (\r\n",
        "    epsilon_max - epsilon_min\r\n",
        ")  # Rate at which to reduce chance of random action being taken\r\n",
        "batch_size = 32  # Size of batch taken from replay buffer\r\n",
        "max_steps_per_episode = 10000\r\n",
        "\r\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\r\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\r\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\r\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\r\n",
        "#env.seed(seed)"
      ],
      "execution_count": 651,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "3iJXNWPXVDVi",
        "outputId": "c1237117-8116-40b0-c77b-5e098ee7eea6"
      },
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\r\n",
        "# improves training time\r\n",
        "optimizer = optim.Adam(model_target.parameters(), lr=0.00025,)\r\n",
        "loss_fn = nn.SmoothL1Loss()\r\n",
        "# Experience replay buffers\r\n",
        "action_history = []\r\n",
        "state_history = []\r\n",
        "state_next_history = []\r\n",
        "rewards_history = []\r\n",
        "done_history = []\r\n",
        "episode_reward_history = []\r\n",
        "running_reward = 0\r\n",
        "episode_count = 0\r\n",
        "frame_count = 0\r\n",
        "# Number of frames to take random action and observe output\r\n",
        "epsilon_random_frames = 50000\r\n",
        "# Number of frames for exploration\r\n",
        "epsilon_greedy_frames = 1000000.0\r\n",
        "# Maximum replay length\r\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\r\n",
        "max_memory_length = 100000\r\n",
        "# Train the model after 4 actions\r\n",
        "update_after_actions = 4\r\n",
        "# How often to update the target network\r\n",
        "update_target_network = 10000\r\n",
        "# Using huber loss for stability\r\n",
        "\r\n",
        "\r\n",
        "while True:  # Run until solved\r\n",
        "    env.reset()\r\n",
        "    state = np.array(env.reset())\r\n",
        "    \r\n",
        "    img = plt.imshow(env.render('rgb_array'))\r\n",
        "    state=np.moveaxis(state, -1, 0)\r\n",
        "    episode_reward = 0\r\n",
        "\r\n",
        "    for timestep in range(1, max_steps_per_episode):\r\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "        display.display(plt.gcf())\r\n",
        "        display.clear_output(wait=True)\r\n",
        "\r\n",
        "        #Adding this line would show the attempts\r\n",
        "        # of the agent in a pop up window.\r\n",
        "        frame_count += 1\r\n",
        "        \r\n",
        "        # Use epsilon-greedy for exploration\r\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\r\n",
        "            # Take random action\r\n",
        "            action = np.random.choice(num_actions)\r\n",
        "        else:\r\n",
        "            # Predict action Q-values\r\n",
        "            # From environment state\r\n",
        "            with torch.no_grad():\r\n",
        "              X_batch = Variable(torch.from_numpy(state))\r\n",
        "              X_batch=X_batch.float()\r\n",
        "              X_batch=torch.unsqueeze(X_batch,dim=0).to(device)\r\n",
        "              action_probs = model(X_batch)\r\n",
        "              # Take best action\r\n",
        "              action = torch.argmax(action_probs[0]).cpu().numpy()\r\n",
        "\r\n",
        "        # Decay probability of taking random action\r\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\r\n",
        "        epsilon = max(epsilon, epsilon_min)\r\n",
        "\r\n",
        "        # Apply the sampled action in our environment\r\n",
        "        state_next, reward, done, _ = env.step(action)\r\n",
        "        state_next = np.array(state_next)\r\n",
        "        state_next=np.moveaxis(state_next, -1, 0)\r\n",
        "        reward = np.array(reward)\r\n",
        "        \r\n",
        "\r\n",
        "        episode_reward += reward\r\n",
        "\r\n",
        "        # Save actions and states in replay buffer\r\n",
        "        action_history.append(action)\r\n",
        "        state_history.append(state)\r\n",
        "\r\n",
        "        state_next_history.append(state_next)\r\n",
        "        done_history.append(done)\r\n",
        "        rewards_history.append(reward)\r\n",
        "        state = state_next\r\n",
        "\r\n",
        "        # Update every fourth frame and once batch size is over 32\r\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\r\n",
        "\r\n",
        "            # Get indices of samples for replay buffers\r\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\r\n",
        "          \r\n",
        "\r\n",
        "            # Using list comprehension to sample from replay buffer\r\n",
        "            state_sample = np.array([state_history[i] for i in indices])\r\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\r\n",
        "            rewards_sample = np.array([rewards_history[i] for i in indices])\r\n",
        "            action_sample = [action_history[i] for i in indices]\r\n",
        "            done_sample = torch.from_numpy(np.array([float(done_history[i]) for i in indices]))\r\n",
        "            \r\n",
        "\r\n",
        "            # Build the updated Q-values for the sampled future states\r\n",
        "            # Use the target model for stability\r\n",
        "            state_next_sample = Variable(torch.from_numpy(state_next_sample))\r\n",
        "            state_next_sample=state_next_sample.float()\r\n",
        "            state_next_sample=state_next_sample.to(device)\r\n",
        "            future_rewards = model_target(state_next_sample)\r\n",
        "            \r\n",
        "\r\n",
        "            # Q value = reward + discount factor * expected future reward\r\n",
        "            \r\n",
        "            updated_q_values = torch.from_numpy(rewards_sample).to(device) + gamma * torch.max(future_rewards,1)[0]\r\n",
        "            \r\n",
        "\r\n",
        "            # If final frame set the last value to -1\r\n",
        "            updated_q_values = updated_q_values * (1 - done_sample.to(device)) - done_sample.to(device)\r\n",
        "            \r\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\r\n",
        "            \r\n",
        "\r\n",
        "            masks =  functional.one_hot(torch.tensor(np.array(action_sample)), num_classes=num_actions)\r\n",
        "            \r\n",
        "\r\n",
        "        \r\n",
        "            # Train the model on the states and updated Q-values\r\n",
        "            \r\n",
        "           \r\n",
        "            state_sample = Variable(torch.from_numpy(state_sample))\r\n",
        "            state_sample=state_sample.float()\r\n",
        "            state_sample=state_sample.to(device)\r\n",
        "            q_values = model(state_sample)\r\n",
        "            \r\n",
        "          \r\n",
        "\r\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\r\n",
        "            q_action = torch.sum(torch.mul(q_values.to(device), masks.to(device)), axis=1)\r\n",
        "            # Calculate loss between new Q-value and old Q-value\r\n",
        "\r\n",
        "            # Backpropagation\r\n",
        "            loss = loss_fn(updated_q_values.double(), q_action.double())\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            for param in model.parameters():\r\n",
        "                param.grad.data.clamp_(-1, 1)\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        if frame_count % update_target_network == 0:\r\n",
        "            # update the the target network with new weights\r\n",
        "            #model_target.set_weights(model.get_weights())\r\n",
        "            model_target.load_state_dict(model.state_dict())\r\n",
        "            # Log details\r\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\r\n",
        "            print(template.format(running_reward, episode_count, frame_count))\r\n",
        "\r\n",
        "        # Limit the state and reward history\r\n",
        "        if len(rewards_history) > max_memory_length:\r\n",
        "            del rewards_history[:1]\r\n",
        "            del state_history[:1]\r\n",
        "            del state_next_history[:1]\r\n",
        "            del action_history[:1]\r\n",
        "            del done_history[:1]\r\n",
        "\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "\r\n",
        "    # Update running reward to check condition for solving\r\n",
        "    episode_reward_history.append(episode_reward)\r\n",
        "    if len(episode_reward_history) > 100:\r\n",
        "        del episode_reward_history[:1]\r\n",
        "    running_reward = np.mean(episode_reward_history)\r\n",
        "\r\n",
        "    episode_count += 1\r\n",
        "\r\n",
        "    if running_reward > 40:  # Condition to consider the task solved\r\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf8cvJR7VH4I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}