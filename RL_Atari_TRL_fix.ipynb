{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAiP1vewBOcw"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.functional as F\r\n",
        "import os"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vmp8-FvfBfJq",
        "outputId": "6122cc86-79cb-4623-985d-1f9db818b0e2"
      },
      "source": [
        "os.getcwd()\r\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UqsAPtBgqn"
      },
      "source": [
        "checkpoint = torch.load('checkpoint.pt')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNAdEeklBhR5",
        "outputId": "dc8fb450-2e09-4df5-f25e-ca1c0ca6d902"
      },
      "source": [
        "for i in checkpoint.values():\r\n",
        "    print(i.size())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 4, 8, 8])\n",
            "torch.Size([32])\n",
            "torch.Size([64, 32, 4, 4])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([512, 4096])\n",
            "torch.Size([512])\n",
            "torch.Size([4, 512])\n",
            "torch.Size([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWezKTkFBwe6"
      },
      "source": [
        "\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOGe75H-ByA5"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "os.environ.setdefault('PATH', '')\r\n",
        "from collections import deque\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import cv2\r\n",
        "cv2.ocl.setUseOpenCL(False)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class NoopResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, noop_max=30):\r\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\r\n",
        "        No-op is assumed to be action 0.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.noop_max = noop_max\r\n",
        "        self.override_num_noops = None\r\n",
        "        self.noop_action = 0\r\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        if self.override_num_noops is not None:\r\n",
        "            noops = self.override_num_noops\r\n",
        "        else:\r\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\r\n",
        "        assert noops > 0\r\n",
        "        obs = None\r\n",
        "        for _ in range(noops):\r\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\r\n",
        "            if done:\r\n",
        "                obs = self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class FireResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(1)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(2)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class EpisodicLifeEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\r\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.lives = 0\r\n",
        "        self.was_real_done  = True\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        obs, reward, done, info = self.env.step(action)\r\n",
        "        self.was_real_done = done\r\n",
        "        # check current lives, make loss of life terminal,\r\n",
        "        # then update lives to handle bonus lives\r\n",
        "        lives = self.env.unwrapped.ale.lives()\r\n",
        "        if lives < self.lives and lives > 0:\r\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\r\n",
        "            # so it's important to keep lives > 0, so that we only reset once\r\n",
        "            # the environment advertises done.\r\n",
        "            done = True\r\n",
        "        self.lives = lives\r\n",
        "        return obs, reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\"Reset only when lives are exhausted.\r\n",
        "        This way all states are still reachable even though lives are episodic,\r\n",
        "        and the learner need not know about any of this behind-the-scenes.\r\n",
        "        \"\"\"\r\n",
        "        if self.was_real_done:\r\n",
        "            obs = self.env.reset(**kwargs)\r\n",
        "        else:\r\n",
        "            # no-op step to advance from terminal/lost life state\r\n",
        "            obs, _, _, _ = self.env.step(0)\r\n",
        "        self.lives = self.env.unwrapped.ale.lives()\r\n",
        "        return obs\r\n",
        "\r\n",
        "class MaxAndSkipEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, skip=4):\r\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        # most recent raw observations (for max pooling across time steps)\r\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\r\n",
        "        self._skip       = skip\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\r\n",
        "        total_reward = 0.0\r\n",
        "        done = None\r\n",
        "        for i in range(self._skip):\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\r\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "        # Note that the observation on the done=True frame\r\n",
        "        # doesn't matter\r\n",
        "        max_frame = self._obs_buffer.max(axis=0)\r\n",
        "\r\n",
        "        return max_frame, total_reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        return self.env.reset(**kwargs)\r\n",
        "\r\n",
        "class ClipRewardEnv(gym.RewardWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.RewardWrapper.__init__(self, env)\r\n",
        "\r\n",
        "    def reward(self, reward):\r\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\r\n",
        "        return np.sign(reward)\r\n",
        "\r\n",
        "\r\n",
        "class WarpFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\r\n",
        "        \"\"\"\r\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\r\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\r\n",
        "        observation should be warped.\r\n",
        "        \"\"\"\r\n",
        "        super().__init__(env)\r\n",
        "        self._width = width\r\n",
        "        self._height = height\r\n",
        "        self._grayscale = grayscale\r\n",
        "        self._key = dict_space_key\r\n",
        "        if self._grayscale:\r\n",
        "            num_colors = 1\r\n",
        "        else:\r\n",
        "            num_colors = 3\r\n",
        "\r\n",
        "        new_space = gym.spaces.Box(\r\n",
        "            low=0,\r\n",
        "            high=255,\r\n",
        "            shape=(self._height, self._width, num_colors),\r\n",
        "            dtype=np.uint8,\r\n",
        "        )\r\n",
        "        if self._key is None:\r\n",
        "            original_space = self.observation_space\r\n",
        "            self.observation_space = new_space\r\n",
        "        else:\r\n",
        "            original_space = self.observation_space.spaces[self._key]\r\n",
        "            self.observation_space.spaces[self._key] = new_space\r\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\r\n",
        "\r\n",
        "    def observation(self, obs):\r\n",
        "        if self._key is None:\r\n",
        "            frame = obs\r\n",
        "        else:\r\n",
        "            frame = obs[self._key]\r\n",
        "\r\n",
        "        if self._grayscale:\r\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\r\n",
        "        frame = cv2.resize(\r\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\r\n",
        "        )\r\n",
        "        if self._grayscale:\r\n",
        "            frame = np.expand_dims(frame, -1)\r\n",
        "\r\n",
        "        if self._key is None:\r\n",
        "            obs = frame\r\n",
        "        else:\r\n",
        "            obs = obs.copy()\r\n",
        "            obs[self._key] = frame\r\n",
        "        return obs\r\n",
        "\r\n",
        "\r\n",
        "class FrameStack(gym.Wrapper):\r\n",
        "    def __init__(self, env, k):\r\n",
        "        \"\"\"Stack k last frames.\r\n",
        "        Returns lazy array, which is much more memory efficient.\r\n",
        "        See Also\r\n",
        "        --------\r\n",
        "        baselines.common.atari_wrappers.LazyFrames\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.k = k\r\n",
        "        self.frames = deque([], maxlen=k)\r\n",
        "        shp = env.observation_space.shape\r\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        ob = self.env.reset()\r\n",
        "        for _ in range(self.k):\r\n",
        "            self.frames.append(ob)\r\n",
        "        return self._get_ob()\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        ob, reward, done, info = self.env.step(action)\r\n",
        "        self.frames.append(ob)\r\n",
        "        return self._get_ob(), reward, done, info\r\n",
        "\r\n",
        "    def _get_ob(self):\r\n",
        "        assert len(self.frames) == self.k\r\n",
        "        return LazyFrames(list(self.frames))\r\n",
        "\r\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.ObservationWrapper.__init__(self, env)\r\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        # careful! This undoes the memory optimization, use\r\n",
        "        # with smaller replay buffers only.\r\n",
        "        return np.array(observation).astype(np.float32) / 255.0\r\n",
        "\r\n",
        "class LazyFrames(object):\r\n",
        "    def __init__(self, frames):\r\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\r\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\r\n",
        "        buffers.\r\n",
        "        This object should only be converted to numpy array before being passed to the model.\r\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\r\n",
        "        self._frames = frames\r\n",
        "        self._out = None\r\n",
        "\r\n",
        "    def _force(self):\r\n",
        "        if self._out is None:\r\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\r\n",
        "            self._frames = None\r\n",
        "        return self._out\r\n",
        "\r\n",
        "    def __array__(self, dtype=None):\r\n",
        "        out = self._force()\r\n",
        "        if dtype is not None:\r\n",
        "            out = out.astype(dtype)\r\n",
        "        return out\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self._force())\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        return self._force()[i]\r\n",
        "\r\n",
        "    def count(self):\r\n",
        "        frames = self._force()\r\n",
        "        return frames.shape[frames.ndim - 1]\r\n",
        "\r\n",
        "    def frame(self, i):\r\n",
        "        return self._force()[..., i]\r\n",
        "\r\n",
        "def make_atari(env_id, max_episode_steps=None):\r\n",
        "    env = gym.make(env_id)\r\n",
        "    assert 'NoFrameskip' in env.spec.id\r\n",
        "    env = NoopResetEnv(env, noop_max=30)\r\n",
        "    env = MaxAndSkipEnv(env, skip=4)\r\n",
        "    if max_episode_steps is not None:\r\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\r\n",
        "    return env\r\n",
        "\r\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\r\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\r\n",
        "    \"\"\"\r\n",
        "    if episode_life:\r\n",
        "        env = EpisodicLifeEnv(env)\r\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\r\n",
        "        env = FireResetEnv(env)\r\n",
        "    env = WarpFrame(env)\r\n",
        "    if scale:\r\n",
        "        env = ScaledFloatFrame(env)\r\n",
        "    if clip_rewards:\r\n",
        "        env = ClipRewardEnv(env)\r\n",
        "    if frame_stack:\r\n",
        "        env = FrameStack(env, 4)\r\n",
        "    return env"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxKaVFOMB3BG"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdRL6hh-B8s4"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG7wgB2NB_MH",
        "outputId": "19534f27-0bd5-4b11-fe06-569f52bdc0f1"
      },
      "source": [
        "!apt-get install python-opengl -y\r\n",
        "\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "\r\n",
        "!pip install piglet\r\n",
        "\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "Display().start()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fc0efa37dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQudhh3QCA4d"
      },
      "source": [
        "import gym\r\n",
        "from IPython import display\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBdIvmZwCDxn"
      },
      "source": [
        "# Use the Baseline Atari environment because of Deepmind helper functions\r\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\r\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\r\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\r\n",
        "#env.seed(seed)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nP9YSJOzCLyE",
        "outputId": "85d17711-f0c5-4dc1-9e3a-477d7ec3f3ae"
      },
      "source": [
        "env.reset()\r\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\r\n",
        "# for _ in range(40):\r\n",
        "#     img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "#     display.display(plt.gcf())\r\n",
        "#     display.clear_output(wait=True)\r\n",
        "#     action = env.action_space.sample()\r\n",
        "#     env.step(action)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARkUlEQVR4nO3df4xdZZ3H8fdnZjq0DsVOqVZSqvQXJrhxK3aBRCDuirWQDZVNZNtsEBfSQkITjG42RczSbNZk13Uwq7uLKYEIq4AsiPIHrnQbg8Hwq9VaCgUpWKRDmWp1mf4Yaafz3T/OmXJnOrdz73PunfuDzyu5uec855x7ngP303vuM+d8ryICM6tOR6M7YNaKHByzBA6OWQIHxyyBg2OWwMExS1C34EhaIelFSbskra/XfswaQfX4O46kTuBXwCeBPcAzwOqIeL7mOzNrgHp94pwH7IqIVyLiCHAfsLJO+zKbcl11et15wGsl83uA88utLMmXL1gz+l1EvGeiBfUKzqQkrQXWNmr/ZhV4tdyCegWnH5hfMn9m3nZcRGwENoI/caz11Os7zjPAEkkLJHUDq4CH67QvsylXl0+ciBiWtA74MdAJ3BkRz9VjX2aNUJfh6Ko70YSnaldddRWLFi2qeP3BwUFuvfXW4/OSuOWWW6ra5wMPPMCOHTuOz59//vlceumlVb3Ghg0bqlp/MnPmzGHdunVVbdPX18eBAwdq2o/xvvzlL9PV9fa/+9/85jfZv39/rXezNSKWTbSgYYMDzW7GjBmcdtppFa8/MjJyQls12wNj3ggA3d3dVb1GPf4R7OjoqPo4JNW8H+PNnDmTadOmHZ/v6Jjai2AcnAo9/vjj/OxnPzs+v3DhQj7zmc9U9Rp9fX0MDw8fn1+zZg2zZ8+uePv+/n6+853vHJ+fPn06N954Y1V9KGp4eJi+vr6TrnPw4MEp6k3jODgVOnjwIAMDA8fne3t7q36NgYGBMcEpna7E0aNHx/RhxowZVfehqIgY04d3KgfHqtLZ2cn1119/0nXuvvtuDh8+PEU9agwHx6rS0dHB2WeffdJ1xn9Xa0ftf4RWyODgIPfcc89J11m9evWUDAg0EwfHTuqPf/wjW7ZsOek6q1atcnBsYosXLx4z5DlnzpyqX2P58uVjhq17enqq2n7WrFmsWLHi+HzpcGy99PT0cNFFF510nXdaaMDBqdjixYtZvHhxode45JJLCm0/a9Ysli9fXug1qtXT0zPl+2wFDk4ZL7zwAn/4wx8qXn9oaOiEtieeeKKqfY7/y/cbb7xR9WvU2tDQUNV9OHLkSJ1687ann356zBnARP/968mX3JiV19yX3EyfPp0FCxY0uhtmY+zcubPssqYIzpw5c1izZk2ju2E2xhe+8IWyy1weyiyBg2OWwMExS+DgmCVIDo6k+ZJ+Iul5Sc9JujFv3yCpX9K2/HFZ7bpr1hyKjKoNA1+MiJ9LmglslbQpX/b1iPha8e6ZNafk4ETEXmBvPn1A0k6yQoRmba8m33EknQV8BHgqb1onabukOyVVf6ukWZMrHBxJpwIPAp+PiEHgNmARsJTsE2nCG9QlrZW0RdKWQ4cOFe2G2ZQqFBxJ08hC892I+D5ARAxExLGIGAFuJyvAfoKI2BgRyyJiWbWX15s1WpFRNQF3ADsj4taS9jNKVrsC2DF+W7NWV2RU7WPAVcCzkrblbV8CVktaCgSwG7iuUA/NmlCRUbXHgYlu/XskvTtmrcFXDpglaIrbCiZzxx138Prrrze6G9ZG5s2bxzXXXJO8fUsE58CBA1Xdxmw2mWrrYY/nUzWzBA6OWQIHxyyBg2OWwMExS+DgmCVwcMwSODhmCRwcswQOjlkCB8csgYNjlsDBMUvg4JglKHxbgaTdwAHgGDAcEcskzQa+B5xFdvv0lRHh+wKsbdTqE+fPI2Jpya9XrQc2R8QSYHM+b9Y26nWqthK4K5++C/h0nfZj1hC1CE4Aj0raKmlt3jY3L5EL8AYwtwb7MWsatbh1+sKI6Jf0XmCTpBdKF0ZETPTjuHnI1gL09rpKrrWWwp84EdGfP+8DHiKr3DkwWpgwf943wXau5Gktq2gJ3J78Jz6Q1AMsJ6vc+TBwdb7a1cAPi+zHrNkUPVWbCzyUVcOlC7gnIv5H0jPA/ZKuBV4Friy4H7OmUig4EfEK8KcTtO8HPlHktc2ama8cMEvQEgUJ/23ZMmYsXtzoblgbGert5dcFtm+J4Jza1cXM7u5Gd8PaSGdXsbe+T9XMEjg4ZgkcHLMEDo5ZgpYYHIjT32JkxuFGd8PaSLxreqHtWyI4vGsYOocb3QtrI3FKsfeTT9XMEjg4ZgkcHLMEDo5ZgpYYHDjaOcKRLg8OWO0Md44U2r4lgnN4+hGi60iju2FtZKjg+8mnamYJHByzBMmnapI+SFatc9RC4B+AWcAa4Ld5+5ci4pHkHpo1oeTgRMSLwFIASZ1AP1mVm78Fvh4RX6tJD82aUK0GBz4BvBwRr+aFO2qrA0Y6TijNZpYsCn5JqVVwVgH3lsyvk/RZYAvwxaIF1wfnDzNt2tEiL2E2xtGjw/Bm+vaFBwckdQOXA/+dN90GLCI7jdsL9JXZbq2kLZK2HDp0qGg3zKZULUbVLgV+HhEDABExEBHHImIEuJ2ssucJXMnTWlktgrOaktO00dK3uSvIKnuatZVC33HysrefBK4raf6qpKVkv2Kwe9wys7ZQtJLnIeD0cW1XFeqRWQtoiWvVNsVcBkeK3epqVurdMYs/K7B9SwRnBBihDn8fsneskYJ/FvS1amYJHByzBA6OWQIHxyxBSwwOHHv6co4e9q8VWO0M9xyBD57w07QVa4ngxP/NJQZnNrob1kbi6AEm+E3nivlUzSyBg2OWwMExS+DgmCVoicGBgb2b2Pdb11Wz2jny3m7gfcnbt0RwXnv1Pn7zm980uhvWRo4MfQC4MXl7n6qZJXBwzBI4OGYJKgqOpDsl7ZO0o6RttqRNkl7Kn3vzdkn6hqRdkrZLOrdenTdrlEo/cb4NrBjXth7YHBFLgM35PGRVb5bkj7Vk5aLM2kpFwYmInwK/H9e8Ergrn74L+HRJ+92ReRKYNa7yjVnLK/IdZ25E7M2n3wDm5tPzgNdK1tuTt43hgoTWymoyOBARQVYOqpptXJDQWlaR4AyMnoLlz6PXaPcD80vWOzNvM2sbRYLzMHB1Pn018MOS9s/mo2sXAG+WnNKZtYWKLrmRdC/wcWCOpD3ALcA/A/dLuhZ4FbgyX/0R4DJgF3CY7PdyzNpKRcGJiNVlFn1ignUDuKFIp8yana8cMEvg4JglcHDMEjg4ZgkcHLMEDo5ZAgfHLIGDY5bAwTFL4OCYJXBwzBI4OGYJHByzBA6OWQIHxyyBg2OWwMExSzBpcMpU8fxXSS/klTofkjQrbz9L0pCkbfnjW/XsvFmjVPKJ821OrOK5CfiTiPgw8CvgppJlL0fE0vxxfW26adZcJg3ORFU8I+LRiBjOZ58kKwFl9o5Ri+841wA/KplfIOkXkh6TdFG5jVzJ01pZoV9kk3QzMAx8N2/aC7w/IvZL+ijwA0kfiojB8dtGxEZgI8D8+fOrqgJq1mjJnziSPgf8JfA3eUkoIuKtiNifT28FXgbOrkE/zZpKUnAkrQD+Hrg8Ig6XtL9HUmc+vZDspz5eqUVHi5gmMU1qdDesjUx6qlamiudNwCnAJmVvyCfzEbSLgX+UdBQYAa6PiPE/DzKlerq62HzJJQyPjHDho482sivWRiYNTpkqnneUWfdB4MGinTJrdr5ywCyBg2OWoNBwdCs4NDzMx3784+p+9cpsEm0fHIBj4dhYbflUzSyBg2OWwMExS+DgmCVwcMwSODhmCRwcswQOjlkCB8csgYNjlsDBMUvg4JglcHDMEqRW8twgqb+kYudlJctukrRL0ouSPlWvjps1UmolT4Cvl1TsfARA0jnAKuBD+Tb/OVq8w6ydJFXyPImVwH15mahfA7uA8wr0z6wpFfmOsy4vun6npN68bR7wWsk6e/K2E7iSp7Wy1ODcBiwClpJV7+yr9gUiYmNELIuIZT09PYndMGuMpOBExEBEHIuIEeB23j4d6wfml6x6Zt5m1lZSK3meUTJ7BTA64vYwsErSKZIWkFXyfLpYF82aT2olz49LWgoEsBu4DiAinpN0P/A8WTH2GyLiWH26btY4Na3kma//FeArRTpl1ux85YBZAgfHLIGDY5bAwTFL4OCYJXBwzBI4OGYJHByzBA6OWQIHxyyBg2OWwMExS+DgmCVwcMwSODhmCRwcswSpBQm/V1KMcLekbXn7WZKGSpZ9q56dN2uUSe8AJStI+O/A3aMNEfHXo9OS+oA3S9Z/OSKW1qqDZs2oklunfyrprImWSRJwJfAXte2WWXMr+h3nImAgIl4qaVsg6ReSHpN0UcHXN2tKlZyqncxq4N6S+b3A+yNiv6SPAj+Q9KGIGBy/oaS1wFqA3t7e8YvNmlryJ46kLuCvgO+NtuU1o/fn01uBl4GzJ9relTytlRU5VbsEeCEi9ow2SHrP6K8TSFpIVpDwlWJdNGs+lQxH3ws8AXxQ0h5J1+aLVjH2NA3gYmB7Pjz9AHB9RFT6SwdmLSO1ICER8bkJ2h4EHizeLbPm5isHzBI4OGYJHByzBA6OWQIHxyyBg2OWwMExS+DgmCVwcMwSFL06uiYGO0fYdNqhssvf7PTPiLaKey68kJld6W+r14eGuO6pp2rYo4mdOjjIssceS96+KYITwFsdUXb5yNR1xQo6vbubd3d3J28/dGxq/pFUBN1vvZW8vU/VzBI4OGYJmuJUzdrHzdu20dWR/u/xVJ2qFeXgWE1t+f074/YrB8fekfoPH+afnn02eXtFlB/Nmird7z413nfBh8suH3jyWY4MHpzCHpkBsDUilk24JCJO+gDmAz8BngeeA27M22cDm4CX8ufevF3AN4BdwHbg3Ar2EX740YSPLeXes5V8ixsGvhgR5wAXADdIOgdYD2yOiCXA5nwe4FKyIh1LyMo/3VbBPsxayqTBiYi9EfHzfPoAsBOYB6wE7spXuwv4dD69Erg7Mk8CsySdUfOemzVQVeOGeSncjwBPAXMjYm++6A1gbj49D3itZLM9eZtZ26h4VE3SqWQVbD4fEYNZ2ehMRISkqGbHpZU8zVpNRZ84kqaRhea7EfH9vHlg9BQsf96Xt/eTDSiMOjNvG6O0kmdq580apZKChALuAHZGxK0lix4Grs6nrwZ+WNL+WWUuAN4sOaUzaw8VDBVfSDY0tx3Ylj8uA04nG017CfhfYHbJcPR/kNWNfhZY5uFoP1r0UXY4uin+AFrt9yOzKVL2D6C+OtosgYNjlsDBMUvg4JglcHDMEjTL/Ti/Aw7lz+1iDu1zPO10LFD58Xyg3IKmGI4GkLSlna4iaKfjaadjgdocj0/VzBI4OGYJmik4GxvdgRprp+Npp2OBGhxP03zHMWslzfSJY9YyGh4cSSskvShpl6T1k2/RfCTtlvSspG2StuRtsyVtkvRS/tzb6H6WI+lOSfsk7Shpm7D/+e0i38j/f22XdG7jej6xMsezQVJ//v9om6TLSpbdlB/Pi5I+VdFOJrvkv54PoJPs9oOFQDfwS+CcRvYp8Th2A3PGtX0VWJ9Prwf+pdH9PEn/LwbOBXZM1n+yW0p+RHb7yAXAU43uf4XHswH4uwnWPSd/350CLMjfj52T7aPRnzjnAbsi4pWIOALcR1bsox2UK2bSdCLip8D4EpwtW4ylzPGUsxK4LyLeiohfk5U1O2+yjRodnHYp7BHAo5K25rUUoHwxk1bRjsVY1uWnl3eWnDonHU+jg9MuLoyIc8lqyt0g6eLShZGdE7Ts8GWr9z93G7AIWArsBfqKvFijg1NRYY9mFxH9+fM+4CGyj/pyxUxaRaFiLM0mIgYi4lhEjAC38/bpWNLxNDo4zwBLJC2Q1A2sIiv20TIk9UiaOToNLAd2UL6YSatoq2Is476HXUH2/wiy41kl6RRJC8gq0D496Qs2wQjIZcCvyEYzbm50fxL6v5BsVOaXZLW1b87bJyxm0owP4F6y05ejZOf415brPwnFWJrkeP4r7+/2PCxnlKx/c348LwKXVrIPXzlglqDRp2pmLcnBMUvg4JglcHDMEjg4ZgkcHLMEDo5ZAgfHLMH/A827Dj/GKnFcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz8hHHkoCcKT"
      },
      "source": [
        "def compute_conv_dim(dim_size, kernel_size, padding, stride):\r\n",
        "    return int((dim_size - kernel_size + 2 * padding) / stride) + 1\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu9aVLZ1COE2"
      },
      "source": [
        "valid_actions = [0,1,2,3]\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self, in_channels, valid_actions, is_target=False):\r\n",
        "        \"\"\"\r\n",
        "        Value estimator for DQN\r\n",
        "        :param in_channels: number of in channels for Conv2d\r\n",
        "        :param valid_actions: all valid actions\r\n",
        "        \"\"\"\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.is_target=is_target\r\n",
        "        # input shape batch_size x in_channels x 84 x 84\r\n",
        "        self.in_channels = in_channels\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=2)\r\n",
        "        self.r1 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\r\n",
        "        self.r2 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3,stride = 2)\r\n",
        "        self.r3 = nn.ReLU()\r\n",
        "\r\n",
        "        self.dense = torch.nn.Linear(64 * 8 * 8, 512)\r\n",
        "        self.r4 = nn.ReLU()\r\n",
        "        self.out = torch.nn.Linear(512, len(valid_actions))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        Calculates probability of each action\r\n",
        "        NOTE: a single discrete state is collection of 4 frames\r\n",
        "        :param x: processed state of shape b x in_channel x 84 x 84\r\n",
        "        :returns tensor of shape [batch_size, NUM_VALID_ACTIONS] (estimated action values)\r\n",
        "        \"\"\"\r\n",
        "        x = self.r1(self.conv1(x))  # b x 32 x 21 x 21\r\n",
        "\r\n",
        "        x = self.r2(self.conv2(x))  # b x 64 x 12 x 12\r\n",
        "\r\n",
        "        x = self.r3(self.conv3(x))\r\n",
        "\r\n",
        "        if self.is_target:\r\n",
        "          x = x.view(x.size(0), -1)  # b x (64 * 12 * 12)\r\n",
        "        dense_out = self.dense(x)  # b x 512\r\n",
        "        dense_out = self.r4(dense_out)\r\n",
        "        output = self.out(dense_out)  # b x VALID_ACTIONS\r\n",
        "        # gather valid action values for each batch based on a.\r\n",
        "        return output\r\n",
        "\r\n",
        "model = Net(in_channels = 4,valid_actions = valid_actions)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iDt1DIICV4K"
      },
      "source": [
        "model.load_state_dict(checkpoint)\r\n",
        "model=model.cuda()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl9fsIfWM_hw",
        "outputId": "d3f9a617-476f-4f6f-a1cf-01ba70da5878"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
            "  (r1): ReLU()\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "  (r2): ReLU()\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "  (r3): ReLU()\n",
            "  (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
            "  (r4): ReLU()\n",
            "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkeXRMa0DiCn"
      },
      "source": [
        "state = np.array(env.reset())"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d8WedhDtj0"
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgx0OC37Duzv"
      },
      "source": [
        "\r\n",
        "x=np.moveaxis(state, -1, 0)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1kXaS9ADwBZ"
      },
      "source": [
        "x=x.reshape(1,4,84,84)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZTjRCssDxK5"
      },
      "source": [
        "#import torch.nn.functional as F\r\n",
        "\r\n",
        "#X_batch = Variable(torch.from_numpy(x))\r\n",
        "\r\n",
        "#X_batch=X_batch.float().cuda()\r\n",
        "#model(X_batch)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlzXihpnEJ5v",
        "outputId": "6798be7e-1a6a-4243-bcd8-e71e80aa9824"
      },
      "source": [
        "\r\n",
        "# The first model makes the predictions for Q-values which are used to\r\n",
        "# make a action.\r\n",
        "model = model\r\n",
        "# Build a target model for the prediction of future rewards.\r\n",
        "# The weights of a target model get updated every 10000 steps thus when the\r\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\r\n",
        "model_target = Net(4,valid_actions, is_target=True)\r\n",
        "device=torch.device('cuda:0')\r\n",
        "model.to(device)\r\n",
        "model_target.to(device)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
              "  (r1): ReLU()\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (r2): ReLU()\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (r3): ReLU()\n",
              "  (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
              "  (r4): ReLU()\n",
              "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US5zrzo1UyNh"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.nn import functional"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGH0vekJVM_H"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "# Configuration paramaters for the whole setup\r\n",
        "seed = 42\r\n",
        "gamma = 0.99  # Discount factor for past rewards\r\n",
        "epsilon = 1.0  # Epsilon greedy parameter\r\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\r\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\r\n",
        "epsilon_interval = (\r\n",
        "    epsilon_max - epsilon_min\r\n",
        ")  # Rate at which to reduce chance of random action being taken\r\n",
        "batch_size = 32  # Size of batch taken from replay buffer\r\n",
        "max_steps_per_episode = 10000\r\n",
        "\r\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\r\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\r\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\r\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\r\n",
        "#env.seed(seed)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqOi7HEXaPdF",
        "outputId": "2edf6428-62a9-4cc4-d59e-21093893ffcd"
      },
      "source": [
        "!pip install tensorly\r\n",
        "!pip install tensorly-torch"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorly in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.18.5)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.3.7)\n",
            "Requirement already satisfied: tensorly-torch in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from tensorly-torch) (1.3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensorly-torch) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorly-torch) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "634VZEoOaPfy"
      },
      "source": [
        "import tltorch"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAJ2JV4ramr1",
        "outputId": "493f64ff-182a-4107-db2b-6f912803131f"
      },
      "source": [
        "model"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
              "  (r1): ReLU()\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (r2): ReLU()\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (r3): ReLU()\n",
              "  (dense): Linear(in_features=4096, out_features=512, bias=True)\n",
              "  (r4): ReLU()\n",
              "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh_YyJ8VaPjp"
      },
      "source": [
        "out_features=512\r\n",
        "in_features = 64 \r\n",
        "spatial_size=8"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMyx2hjaPqt",
        "outputId": "a1d025e5-efa2-4065-ff4b-76c70f0b9c60"
      },
      "source": [
        "trl = tltorch.TuckerTRL((in_features, spatial_size, spatial_size), # input shape\r\n",
        "                (out_features, ), # output shape\r\n",
        "                rank=(in_features, 1, 1, out_features),#full rank\r\n",
        "                project_input=True) # More efficient computation 2))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa75kTNjakpz"
      },
      "source": [
        "model.dense = trl.cuda()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-dzw0CKakt2",
        "outputId": "93c50382-f262-4b2b-f635-b77e41253128"
      },
      "source": [
        "model"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(2, 2))\n",
              "  (r1): ReLU()\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (r2): ReLU()\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (r3): ReLU()\n",
              "  (dense): TuckerTRL(\n",
              "    (factors): ParameterList(\n",
              "        (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]\n",
              "        (1): Parameter containing: [torch.cuda.FloatTensor of size 8x1 (GPU 0)]\n",
              "        (2): Parameter containing: [torch.cuda.FloatTensor of size 8x1 (GPU 0)]\n",
              "        (3): Parameter containing: [torch.cuda.FloatTensor of size 512x512 (GPU 0)]\n",
              "    )\n",
              "  )\n",
              "  (r4): ReLU()\n",
              "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "3iJXNWPXVDVi",
        "outputId": "43188dd7-b510-4eeb-c67e-b7bcd4dc7c2c"
      },
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\r\n",
        "# improves training time\r\n",
        "optimizer = optim.Adam(model_target.parameters(), lr=0.00025,)\r\n",
        "loss_fn = nn.SmoothL1Loss()\r\n",
        "\r\n",
        "num_actions=4\r\n",
        "# Experience replay buffers\r\n",
        "action_history = []\r\n",
        "state_history = []\r\n",
        "state_next_history = []\r\n",
        "rewards_history = []\r\n",
        "done_history = []\r\n",
        "episode_reward_history = []\r\n",
        "running_reward = 0\r\n",
        "episode_count = 0\r\n",
        "frame_count = 0\r\n",
        "# Number of frames to take random action and observe output\r\n",
        "epsilon_random_frames = 50000\r\n",
        "# Number of frames for exploration\r\n",
        "epsilon_greedy_frames = 1000000.0\r\n",
        "# Maximum replay length\r\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\r\n",
        "max_memory_length = 100000\r\n",
        "# Train the model after 4 actions\r\n",
        "update_after_actions = 4\r\n",
        "# How often to update the target network\r\n",
        "update_target_network = 10000\r\n",
        "# Using huber loss for stability\r\n",
        "\r\n",
        "\r\n",
        "while True:  # Run until solved\r\n",
        "    env.reset()\r\n",
        "    state = np.array(env.reset())\r\n",
        "    \r\n",
        "    img = plt.imshow(env.render('rgb_array'))\r\n",
        "    state=np.moveaxis(state, -1, 0)\r\n",
        "    episode_reward = 0\r\n",
        "\r\n",
        "    for timestep in range(1, max_steps_per_episode):\r\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "        display.display(plt.gcf())\r\n",
        "        display.clear_output(wait=True)\r\n",
        "\r\n",
        "        #Adding this line would show the attempts\r\n",
        "        # of the agent in a pop up window.\r\n",
        "        frame_count += 1\r\n",
        "        \r\n",
        "        # Use epsilon-greedy for exploration\r\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\r\n",
        "            # Take random action\r\n",
        "            action = np.random.choice(num_actions)\r\n",
        "        else:\r\n",
        "            # Predict action Q-values\r\n",
        "            # From environment state\r\n",
        "            with torch.no_grad():\r\n",
        "              X_batch = Variable(torch.from_numpy(state))\r\n",
        "              X_batch=X_batch.float()\r\n",
        "              X_batch=torch.unsqueeze(X_batch,dim=0).to(device)\r\n",
        "              action_probs = model(X_batch)\r\n",
        "              # Take best action\r\n",
        "              action = torch.argmax(action_probs[0]).cpu().numpy()\r\n",
        "\r\n",
        "        # Decay probability of taking random action\r\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\r\n",
        "        epsilon = max(epsilon, epsilon_min)\r\n",
        "\r\n",
        "        # Apply the sampled action in our environment\r\n",
        "        state_next, reward, done, _ = env.step(action)\r\n",
        "        state_next = np.array(state_next)\r\n",
        "        state_next=np.moveaxis(state_next, -1, 0)\r\n",
        "        reward = np.array(reward)\r\n",
        "        \r\n",
        "\r\n",
        "        episode_reward += reward\r\n",
        "\r\n",
        "        # Save actions and states in replay buffer\r\n",
        "        action_history.append(action)\r\n",
        "        state_history.append(state)\r\n",
        "\r\n",
        "        state_next_history.append(state_next)\r\n",
        "        done_history.append(done)\r\n",
        "        rewards_history.append(reward)\r\n",
        "        state = state_next\r\n",
        "\r\n",
        "        # Update every fourth frame and once batch size is over 32\r\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\r\n",
        "\r\n",
        "            # Get indices of samples for replay buffers\r\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\r\n",
        "          \r\n",
        "\r\n",
        "            # Using list comprehension to sample from replay buffer\r\n",
        "            state_sample = np.array([state_history[i] for i in indices])\r\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\r\n",
        "            rewards_sample = np.array([rewards_history[i] for i in indices])\r\n",
        "            action_sample = [action_history[i] for i in indices]\r\n",
        "            done_sample = torch.from_numpy(np.array([float(done_history[i]) for i in indices]))\r\n",
        "            \r\n",
        "\r\n",
        "            # Build the updated Q-values for the sampled future states\r\n",
        "            # Use the target model for stability\r\n",
        "            state_next_sample = Variable(torch.from_numpy(state_next_sample))\r\n",
        "            state_next_sample=state_next_sample.float()\r\n",
        "            state_next_sample=state_next_sample.to(device)\r\n",
        "            future_rewards = model_target(state_next_sample)\r\n",
        "            \r\n",
        "\r\n",
        "            # Q value = reward + discount factor * expected future reward\r\n",
        "            \r\n",
        "            updated_q_values = torch.from_numpy(rewards_sample).to(device) + gamma * torch.max(future_rewards,1)[0]\r\n",
        "            \r\n",
        "\r\n",
        "            # If final frame set the last value to -1\r\n",
        "            updated_q_values = updated_q_values * (1 - done_sample.to(device)) - done_sample.to(device)\r\n",
        "            \r\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\r\n",
        "            \r\n",
        "\r\n",
        "            masks =  functional.one_hot(torch.tensor(np.array(action_sample)), num_classes=num_actions)\r\n",
        "            \r\n",
        "\r\n",
        "        \r\n",
        "            # Train the model on the states and updated Q-values\r\n",
        "            \r\n",
        "           \r\n",
        "            state_sample = Variable(torch.from_numpy(state_sample))\r\n",
        "            state_sample=state_sample.float()\r\n",
        "            state_sample=state_sample.to(device)\r\n",
        "            q_values = model(state_sample)\r\n",
        "            \r\n",
        "          \r\n",
        "\r\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\r\n",
        "            q_action = torch.sum(torch.mul(q_values.to(device), masks.to(device)), axis=1)\r\n",
        "            # Calculate loss between new Q-value and old Q-value\r\n",
        "\r\n",
        "            # Backpropagation\r\n",
        "            loss = loss_fn(updated_q_values.double(), q_action.double())\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            for param in model.parameters():\r\n",
        "                param.grad.data.clamp_(-1, 1)\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        if frame_count % update_target_network == 0:\r\n",
        "            # update the the target network with new weights\r\n",
        "            #model_target.set_weights(model.get_weights())\r\n",
        "            model_target.load_state_dict(model.state_dict())\r\n",
        "            # Log details\r\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\r\n",
        "            print(template.format(running_reward, episode_count, frame_count))\r\n",
        "\r\n",
        "        # Limit the state and reward history\r\n",
        "        if len(rewards_history) > max_memory_length:\r\n",
        "            del rewards_history[:1]\r\n",
        "            del state_history[:1]\r\n",
        "            del state_next_history[:1]\r\n",
        "            del action_history[:1]\r\n",
        "            del done_history[:1]\r\n",
        "\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "\r\n",
        "    # Update running reward to check condition for solving\r\n",
        "    episode_reward_history.append(episode_reward)\r\n",
        "    if len(episode_reward_history) > 100:\r\n",
        "        del episode_reward_history[:1]\r\n",
        "    running_reward = np.mean(episode_reward_history)\r\n",
        "\r\n",
        "    episode_count += 1\r\n",
        "\r\n",
        "    if running_reward > 40:  # Condition to consider the task solved\r\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw_vKqbjkjbm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}