{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Atari_Torch_render.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2BfFaQLUaFNyM61Kgkw1Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1eg1on/NLA-RL/blob/main/RL_Atari_Torch_render.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdxM6Upi-RZT"
      },
      "source": [
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGN04CAH2aVT"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "os.environ.setdefault('PATH', '')\r\n",
        "from collections import deque\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import cv2\r\n",
        "cv2.ocl.setUseOpenCL(False)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class NoopResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, noop_max=30):\r\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\r\n",
        "        No-op is assumed to be action 0.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.noop_max = noop_max\r\n",
        "        self.override_num_noops = None\r\n",
        "        self.noop_action = 0\r\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        if self.override_num_noops is not None:\r\n",
        "            noops = self.override_num_noops\r\n",
        "        else:\r\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\r\n",
        "        assert noops > 0\r\n",
        "        obs = None\r\n",
        "        for _ in range(noops):\r\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\r\n",
        "            if done:\r\n",
        "                obs = self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class FireResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(1)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(2)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class EpisodicLifeEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\r\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.lives = 0\r\n",
        "        self.was_real_done  = True\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        obs, reward, done, info = self.env.step(action)\r\n",
        "        self.was_real_done = done\r\n",
        "        # check current lives, make loss of life terminal,\r\n",
        "        # then update lives to handle bonus lives\r\n",
        "        lives = self.env.unwrapped.ale.lives()\r\n",
        "        if lives < self.lives and lives > 0:\r\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\r\n",
        "            # so it's important to keep lives > 0, so that we only reset once\r\n",
        "            # the environment advertises done.\r\n",
        "            done = True\r\n",
        "        self.lives = lives\r\n",
        "        return obs, reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\"Reset only when lives are exhausted.\r\n",
        "        This way all states are still reachable even though lives are episodic,\r\n",
        "        and the learner need not know about any of this behind-the-scenes.\r\n",
        "        \"\"\"\r\n",
        "        if self.was_real_done:\r\n",
        "            obs = self.env.reset(**kwargs)\r\n",
        "        else:\r\n",
        "            # no-op step to advance from terminal/lost life state\r\n",
        "            obs, _, _, _ = self.env.step(0)\r\n",
        "        self.lives = self.env.unwrapped.ale.lives()\r\n",
        "        return obs\r\n",
        "\r\n",
        "class MaxAndSkipEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, skip=4):\r\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        # most recent raw observations (for max pooling across time steps)\r\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\r\n",
        "        self._skip       = skip\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\r\n",
        "        total_reward = 0.0\r\n",
        "        done = None\r\n",
        "        for i in range(self._skip):\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\r\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "        # Note that the observation on the done=True frame\r\n",
        "        # doesn't matter\r\n",
        "        max_frame = self._obs_buffer.max(axis=0)\r\n",
        "\r\n",
        "        return max_frame, total_reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        return self.env.reset(**kwargs)\r\n",
        "\r\n",
        "class ClipRewardEnv(gym.RewardWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.RewardWrapper.__init__(self, env)\r\n",
        "\r\n",
        "    def reward(self, reward):\r\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\r\n",
        "        return np.sign(reward)\r\n",
        "\r\n",
        "\r\n",
        "class WarpFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\r\n",
        "        \"\"\"\r\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\r\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\r\n",
        "        observation should be warped.\r\n",
        "        \"\"\"\r\n",
        "        super().__init__(env)\r\n",
        "        self._width = width\r\n",
        "        self._height = height\r\n",
        "        self._grayscale = grayscale\r\n",
        "        self._key = dict_space_key\r\n",
        "        if self._grayscale:\r\n",
        "            num_colors = 1\r\n",
        "        else:\r\n",
        "            num_colors = 3\r\n",
        "\r\n",
        "        new_space = gym.spaces.Box(\r\n",
        "            low=0,\r\n",
        "            high=255,\r\n",
        "            shape=(self._height, self._width, num_colors),\r\n",
        "            dtype=np.uint8,\r\n",
        "        )\r\n",
        "        if self._key is None:\r\n",
        "            original_space = self.observation_space\r\n",
        "            self.observation_space = new_space\r\n",
        "        else:\r\n",
        "            original_space = self.observation_space.spaces[self._key]\r\n",
        "            self.observation_space.spaces[self._key] = new_space\r\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\r\n",
        "\r\n",
        "    def observation(self, obs):\r\n",
        "        if self._key is None:\r\n",
        "            frame = obs\r\n",
        "        else:\r\n",
        "            frame = obs[self._key]\r\n",
        "\r\n",
        "        if self._grayscale:\r\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\r\n",
        "        frame = cv2.resize(\r\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\r\n",
        "        )\r\n",
        "        if self._grayscale:\r\n",
        "            frame = np.expand_dims(frame, -1)\r\n",
        "\r\n",
        "        if self._key is None:\r\n",
        "            obs = frame\r\n",
        "        else:\r\n",
        "            obs = obs.copy()\r\n",
        "            obs[self._key] = frame\r\n",
        "        return obs\r\n",
        "\r\n",
        "\r\n",
        "class FrameStack(gym.Wrapper):\r\n",
        "    def __init__(self, env, k):\r\n",
        "        \"\"\"Stack k last frames.\r\n",
        "        Returns lazy array, which is much more memory efficient.\r\n",
        "        See Also\r\n",
        "        --------\r\n",
        "        baselines.common.atari_wrappers.LazyFrames\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.k = k\r\n",
        "        self.frames = deque([], maxlen=k)\r\n",
        "        shp = env.observation_space.shape\r\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        ob = self.env.reset()\r\n",
        "        for _ in range(self.k):\r\n",
        "            self.frames.append(ob)\r\n",
        "        return self._get_ob()\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        ob, reward, done, info = self.env.step(action)\r\n",
        "        self.frames.append(ob)\r\n",
        "        return self._get_ob(), reward, done, info\r\n",
        "\r\n",
        "    def _get_ob(self):\r\n",
        "        assert len(self.frames) == self.k\r\n",
        "        return LazyFrames(list(self.frames))\r\n",
        "\r\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.ObservationWrapper.__init__(self, env)\r\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        # careful! This undoes the memory optimization, use\r\n",
        "        # with smaller replay buffers only.\r\n",
        "        return np.array(observation).astype(np.float32) / 255.0\r\n",
        "\r\n",
        "class LazyFrames(object):\r\n",
        "    def __init__(self, frames):\r\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\r\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\r\n",
        "        buffers.\r\n",
        "        This object should only be converted to numpy array before being passed to the model.\r\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\r\n",
        "        self._frames = frames\r\n",
        "        self._out = None\r\n",
        "\r\n",
        "    def _force(self):\r\n",
        "        if self._out is None:\r\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\r\n",
        "            self._frames = None\r\n",
        "        return self._out\r\n",
        "\r\n",
        "    def __array__(self, dtype=None):\r\n",
        "        out = self._force()\r\n",
        "        if dtype is not None:\r\n",
        "            out = out.astype(dtype)\r\n",
        "        return out\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self._force())\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        return self._force()[i]\r\n",
        "\r\n",
        "    def count(self):\r\n",
        "        frames = self._force()\r\n",
        "        return frames.shape[frames.ndim - 1]\r\n",
        "\r\n",
        "    def frame(self, i):\r\n",
        "        return self._force()[..., i]\r\n",
        "\r\n",
        "def make_atari(env_id, max_episode_steps=None):\r\n",
        "    env = gym.make(env_id)\r\n",
        "    assert 'NoFrameskip' in env.spec.id\r\n",
        "    env = NoopResetEnv(env, noop_max=30)\r\n",
        "    env = MaxAndSkipEnv(env, skip=4)\r\n",
        "    if max_episode_steps is not None:\r\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\r\n",
        "    return env\r\n",
        "\r\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\r\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\r\n",
        "    \"\"\"\r\n",
        "    if episode_life:\r\n",
        "        env = EpisodicLifeEnv(env)\r\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\r\n",
        "        env = FireResetEnv(env)\r\n",
        "    env = WarpFrame(env)\r\n",
        "    if scale:\r\n",
        "        env = ScaledFloatFrame(env)\r\n",
        "    if clip_rewards:\r\n",
        "        env = ClipRewardEnv(env)\r\n",
        "    if frame_stack:\r\n",
        "        env = FrameStack(env, 4)\r\n",
        "    return env"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4MJNY_Ky-x8"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1UIlQf3zJEV"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9lI4g6bzPhk",
        "outputId": "566411a4-a9ea-47b5-f065-f51d7a4e1a90"
      },
      "source": [
        "!apt-get install python-opengl -y\r\n",
        "\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "\r\n",
        "!pip install piglet\r\n",
        "\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "Display().start()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f09ce4f6978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec5phZ-UzSWE"
      },
      "source": [
        "import gym\r\n",
        "from IPython import display\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2y90pJGB-o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9948105e-a95e-4b79-dd4e-f8568d77b5a5"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "# Configuration paramaters for the whole setup\r\n",
        "seed = 42\r\n",
        "gamma = 0.99  # Discount factor for past rewards\r\n",
        "epsilon = 1.0  # Epsilon greedy parameter\r\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\r\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\r\n",
        "epsilon_interval = (\r\n",
        "    epsilon_max - epsilon_min\r\n",
        ")  # Rate at which to reduce chance of random action being taken\r\n",
        "batch_size = 32  # Size of batch taken from replay buffer\r\n",
        "max_steps_per_episode = 10000\r\n",
        "\r\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\r\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\r\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\r\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\r\n",
        "env.seed(seed)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42, 742738649]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Jv83aecfzX17",
        "outputId": "7a26aa61-4d6d-4fcb-c4ab-094386d3b978"
      },
      "source": [
        "env.reset()\r\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\r\n",
        "# for _ in range(40):\r\n",
        "#     img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "#     display.display(plt.gcf())\r\n",
        "#     display.clear_output(wait=True)\r\n",
        "#     action = env.action_space.sample()\r\n",
        "#     env.step(action)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hSWVkqGk0-k"
      },
      "source": [
        "num_actions = 4\r\n",
        "\r\n",
        "\r\n",
        "# def create_q_model():\r\n",
        "#     # Network defined by the Deepmind paper\r\n",
        "#     inputs = layers.Input(shape=(84, 84, 4,))\r\n",
        "\r\n",
        "#     # Convolutions on the frames on the screen\r\n",
        "#     layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\r\n",
        "#     layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\r\n",
        "#     layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\r\n",
        "\r\n",
        "#     layer4 = layers.Flatten()(layer3)\r\n",
        "\r\n",
        "#     layer5 = layers.Dense(512, activation=\"relu\")(layer4)\r\n",
        "#     action = layers.Dense(num_actions, activation=\"linear\")(layer5)\r\n",
        "\r\n",
        "#     return keras.Model(inputs=inputs, outputs=action)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYVtTyyTlWYX"
      },
      "source": [
        "def compute_conv_dim(dim_size, kernel_size, padding, stride):\r\n",
        "    return int((dim_size - kernel_size + 2 * padding) / stride + 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpCIADTAnhz2"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahg5RaP2lIPM"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "\r\n",
        "      def __init__(self,out = num_actions):\r\n",
        "              super(Net, self).__init__()\r\n",
        "\r\n",
        "              self.out = out # the action space based output\r\n",
        "\r\n",
        "              self.conv1 = nn.Conv2d(in_channels=4,out_channels=32,kernel_size = (8,8),stride = 4)\r\n",
        "              self.conv_out = compute_conv_dim(84, kernel_size=8, padding=0, stride=4)\r\n",
        "\r\n",
        "              self.conv2 = nn.Conv2d(32,64,kernel_size = (4,4),stride = 2)\r\n",
        "              self.conv_out = compute_conv_dim(self.conv_out, kernel_size=4, padding=0, stride=2)\r\n",
        "\r\n",
        "              self.conv3 = nn.Conv2d(64,64,kernel_size = (4,4),stride = 2)\r\n",
        "\r\n",
        "              self.conv_out = compute_conv_dim(self.conv_out, kernel_size=4, padding=0, stride=2)\r\n",
        "\r\n",
        "              self.l1_in_features = 64 * self.conv_out * self.conv_out\r\n",
        "\r\n",
        "              self.fc1 = nn.Linear(in_features=self.l1_in_features, out_features=512) \r\n",
        "              self.fc2 = nn.Linear(512,out_features = self.out) \r\n",
        "\r\n",
        "      def forward(self, x): \r\n",
        "\r\n",
        "              x = F.relu(self.conv1(x))\r\n",
        "              x = F.relu(self.conv2(x))\r\n",
        "              x = F.relu(self.conv3(x))\r\n",
        "              x = x.view(-1, self.l1_in_features)\r\n",
        "              x = F.relu(self.fc1(x))        \r\n",
        "              return self.fc2(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb31lngIoKVS"
      },
      "source": [
        "model_torch=Net()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJQh8jYLoPI3"
      },
      "source": [
        "state = np.array(env.reset())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3quEtk2ou8b"
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQM17JJpqkHg"
      },
      "source": [
        "x=np.moveaxis(state, -1, 0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Xliqw9qOjM"
      },
      "source": [
        "x=x.reshape(1,4,84,84)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SgLvrTooQfT",
        "outputId": "a20cbed5-548b-4d67-f63a-e3dca10dea07"
      },
      "source": [
        "X_batch = Variable(torch.from_numpy(x))\r\n",
        "\r\n",
        "X_batch=X_batch.float()\r\n",
        "model_torch(X_batch)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0282,  0.0423,  0.0382, -0.0164]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyTMyxTs2r9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4762f1ba-db1a-4a40-eeb1-adcf10d34ecb"
      },
      "source": [
        "# The first model makes the predictions for Q-values which are used to\r\n",
        "# make a action.\r\n",
        "model = Net()\r\n",
        "# Build a target model for the prediction of future rewards.\r\n",
        "# The weights of a target model get updated every 10000 steps thus when the\r\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\r\n",
        "model_target = Net()\r\n",
        "device=torch.device('cuda:0')\r\n",
        "model.to(device)\r\n",
        "model_target.to(device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (fc1): Linear(in_features=576, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-BwpRAjgBQE"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.nn import functional"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1DQiJKG261-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "be670f12-3dd4-4f65-c175-e3392dd45122"
      },
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\r\n",
        "# improves training time\r\n",
        "optimizer = optim.Adam(model_target.parameters(), lr=0.00025,)\r\n",
        "loss_fn = nn.SmoothL1Loss()\r\n",
        "# Experience replay buffers\r\n",
        "action_history = []\r\n",
        "state_history = []\r\n",
        "state_next_history = []\r\n",
        "rewards_history = []\r\n",
        "done_history = []\r\n",
        "episode_reward_history = []\r\n",
        "running_reward = 0\r\n",
        "episode_count = 0\r\n",
        "frame_count = 0\r\n",
        "# Number of frames to take random action and observe output\r\n",
        "epsilon_random_frames = 50000\r\n",
        "# Number of frames for exploration\r\n",
        "epsilon_greedy_frames = 1000000.0\r\n",
        "# Maximum replay length\r\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\r\n",
        "max_memory_length = 100000\r\n",
        "# Train the model after 4 actions\r\n",
        "update_after_actions = 4\r\n",
        "# How often to update the target network\r\n",
        "update_target_network = 10000\r\n",
        "# Using huber loss for stability\r\n",
        "\r\n",
        "\r\n",
        "while True:  # Run until solved\r\n",
        "    env.reset()\r\n",
        "    state = np.array(env.reset())\r\n",
        "    \r\n",
        "    img = plt.imshow(env.render('rgb_array'))\r\n",
        "    state=np.moveaxis(state, -1, 0)\r\n",
        "    episode_reward = 0\r\n",
        "\r\n",
        "    for timestep in range(1, max_steps_per_episode):\r\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "        display.display(plt.gcf())\r\n",
        "        display.clear_output(wait=True)\r\n",
        "\r\n",
        "        #Adding this line would show the attempts\r\n",
        "        # of the agent in a pop up window.\r\n",
        "        frame_count += 1\r\n",
        "        epsilon=0\r\n",
        "        # Use epsilon-greedy for exploration\r\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\r\n",
        "            # Take random action\r\n",
        "            action = np.random.choice(num_actions)\r\n",
        "        else:\r\n",
        "            # Predict action Q-values\r\n",
        "            # From environment state\r\n",
        "            with torch.no_grad():\r\n",
        "              X_batch = Variable(torch.from_numpy(state))\r\n",
        "              X_batch=X_batch.float()\r\n",
        "              X_batch=torch.unsqueeze(X_batch,dim=0).to(device)\r\n",
        "              action_probs = model(X_batch)\r\n",
        "              # Take best action\r\n",
        "              action = torch.argmax(action_probs[0]).cpu().numpy()\r\n",
        "\r\n",
        "        # Decay probability of taking random action\r\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\r\n",
        "        epsilon = max(epsilon, epsilon_min)\r\n",
        "\r\n",
        "        # Apply the sampled action in our environment\r\n",
        "        state_next, reward, done, _ = env.step(action)\r\n",
        "        state_next = np.array(state_next)\r\n",
        "        state_next=np.moveaxis(state_next, -1, 0)\r\n",
        "        reward = np.array(reward)\r\n",
        "        \r\n",
        "\r\n",
        "        episode_reward += reward\r\n",
        "\r\n",
        "        # Save actions and states in replay buffer\r\n",
        "        action_history.append(action)\r\n",
        "        state_history.append(state)\r\n",
        "\r\n",
        "        state_next_history.append(state_next)\r\n",
        "        done_history.append(done)\r\n",
        "        rewards_history.append(reward)\r\n",
        "        state = state_next\r\n",
        "\r\n",
        "        # Update every fourth frame and once batch size is over 32\r\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\r\n",
        "\r\n",
        "            # Get indices of samples for replay buffers\r\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\r\n",
        "          \r\n",
        "\r\n",
        "            # Using list comprehension to sample from replay buffer\r\n",
        "            state_sample = np.array([state_history[i] for i in indices])\r\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\r\n",
        "            rewards_sample = np.array([rewards_history[i] for i in indices])\r\n",
        "            action_sample = [action_history[i] for i in indices]\r\n",
        "            done_sample = torch.from_numpy(np.array([float(done_history[i]) for i in indices]))\r\n",
        "            \r\n",
        "\r\n",
        "            # Build the updated Q-values for the sampled future states\r\n",
        "            # Use the target model for stability\r\n",
        "            state_next_sample = Variable(torch.from_numpy(state_next_sample))\r\n",
        "            state_next_sample=state_next_sample.float()\r\n",
        "            state_next_sample=state_next_sample.to(device)\r\n",
        "            future_rewards = model_target(state_next_sample)\r\n",
        "            \r\n",
        "\r\n",
        "            # Q value = reward + discount factor * expected future reward\r\n",
        "            \r\n",
        "            updated_q_values = torch.from_numpy(rewards_sample).to(device) + gamma * torch.max(future_rewards,1)[0]\r\n",
        "            \r\n",
        "\r\n",
        "            # If final frame set the last value to -1\r\n",
        "            updated_q_values = updated_q_values * (1 - done_sample.to(device)) - done_sample.to(device)\r\n",
        "            \r\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\r\n",
        "            \r\n",
        "\r\n",
        "            masks =  functional.one_hot(torch.tensor(np.array(action_sample)), num_classes=num_actions)\r\n",
        "            \r\n",
        "\r\n",
        "        \r\n",
        "            # Train the model on the states and updated Q-values\r\n",
        "            \r\n",
        "           \r\n",
        "            state_sample = Variable(torch.from_numpy(state_sample))\r\n",
        "            state_sample=state_sample.float()\r\n",
        "            state_sample=state_sample.to(device)\r\n",
        "            q_values = model(state_sample)\r\n",
        "            \r\n",
        "          \r\n",
        "\r\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\r\n",
        "            q_action = torch.sum(torch.mul(q_values.to(device), masks.to(device)), axis=1)\r\n",
        "            # Calculate loss between new Q-value and old Q-value\r\n",
        "\r\n",
        "            # Backpropagation\r\n",
        "            loss = loss_fn(updated_q_values.double(), q_action.double())\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            for param in model.parameters():\r\n",
        "                param.grad.data.clamp_(-1, 1)\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        if frame_count % update_target_network == 0:\r\n",
        "            # update the the target network with new weights\r\n",
        "            #model_target.set_weights(model.get_weights())\r\n",
        "            model_target.load_state_dict(model.state_dict())\r\n",
        "            # Log details\r\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\r\n",
        "            print(template.format(running_reward, episode_count, frame_count))\r\n",
        "\r\n",
        "        # Limit the state and reward history\r\n",
        "        if len(rewards_history) > max_memory_length:\r\n",
        "            del rewards_history[:1]\r\n",
        "            del state_history[:1]\r\n",
        "            del state_next_history[:1]\r\n",
        "            del action_history[:1]\r\n",
        "            del done_history[:1]\r\n",
        "\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "\r\n",
        "    # Update running reward to check condition for solving\r\n",
        "    episode_reward_history.append(episode_reward)\r\n",
        "    if len(episode_reward_history) > 100:\r\n",
        "        del episode_reward_history[:1]\r\n",
        "    running_reward = np.mean(episode_reward_history)\r\n",
        "\r\n",
        "    episode_count += 1\r\n",
        "\r\n",
        "    if running_reward > 40:  # Condition to consider the task solved\r\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\r\n",
        "        break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-03265e353368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just update the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         }\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpil_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    392\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 626\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             magnification, unsampled=unsampled)\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scalar_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 output_alpha = _resample(  # resample alpha channel\n\u001b[0;32m--> 522\u001b[0;31m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[0m\u001b[1;32m    523\u001b[0m                 output = _resample(  # resample rgb channels\n\u001b[1;32m    524\u001b[0m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     image_obj.get_filterrad())\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlZ6HDOo4x1n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eoQk3eW2_Tl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}