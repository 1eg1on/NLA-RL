{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Atari_Torch_render",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnCphDvV45+66NZRov78JX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1eg1on/NLA-RL/blob/main/RL_Atari_Torch_render.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdxM6Upi-RZT"
      },
      "source": [
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGN04CAH2aVT"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "os.environ.setdefault('PATH', '')\r\n",
        "from collections import deque\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import cv2\r\n",
        "cv2.ocl.setUseOpenCL(False)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class NoopResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, noop_max=30):\r\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\r\n",
        "        No-op is assumed to be action 0.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.noop_max = noop_max\r\n",
        "        self.override_num_noops = None\r\n",
        "        self.noop_action = 0\r\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        if self.override_num_noops is not None:\r\n",
        "            noops = self.override_num_noops\r\n",
        "        else:\r\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\r\n",
        "        assert noops > 0\r\n",
        "        obs = None\r\n",
        "        for _ in range(noops):\r\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\r\n",
        "            if done:\r\n",
        "                obs = self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class FireResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(1)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        obs, _, done, _ = self.env.step(2)\r\n",
        "        if done:\r\n",
        "            self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)\r\n",
        "\r\n",
        "class EpisodicLifeEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\r\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.lives = 0\r\n",
        "        self.was_real_done  = True\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        obs, reward, done, info = self.env.step(action)\r\n",
        "        self.was_real_done = done\r\n",
        "        # check current lives, make loss of life terminal,\r\n",
        "        # then update lives to handle bonus lives\r\n",
        "        lives = self.env.unwrapped.ale.lives()\r\n",
        "        if lives < self.lives and lives > 0:\r\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\r\n",
        "            # so it's important to keep lives > 0, so that we only reset once\r\n",
        "            # the environment advertises done.\r\n",
        "            done = True\r\n",
        "        self.lives = lives\r\n",
        "        return obs, reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\"Reset only when lives are exhausted.\r\n",
        "        This way all states are still reachable even though lives are episodic,\r\n",
        "        and the learner need not know about any of this behind-the-scenes.\r\n",
        "        \"\"\"\r\n",
        "        if self.was_real_done:\r\n",
        "            obs = self.env.reset(**kwargs)\r\n",
        "        else:\r\n",
        "            # no-op step to advance from terminal/lost life state\r\n",
        "            obs, _, _, _ = self.env.step(0)\r\n",
        "        self.lives = self.env.unwrapped.ale.lives()\r\n",
        "        return obs\r\n",
        "\r\n",
        "class MaxAndSkipEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, skip=4):\r\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        # most recent raw observations (for max pooling across time steps)\r\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\r\n",
        "        self._skip       = skip\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\r\n",
        "        total_reward = 0.0\r\n",
        "        done = None\r\n",
        "        for i in range(self._skip):\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\r\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "        # Note that the observation on the done=True frame\r\n",
        "        # doesn't matter\r\n",
        "        max_frame = self._obs_buffer.max(axis=0)\r\n",
        "\r\n",
        "        return max_frame, total_reward, done, info\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        return self.env.reset(**kwargs)\r\n",
        "\r\n",
        "class ClipRewardEnv(gym.RewardWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.RewardWrapper.__init__(self, env)\r\n",
        "\r\n",
        "    def reward(self, reward):\r\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\r\n",
        "        return np.sign(reward)\r\n",
        "\r\n",
        "\r\n",
        "class WarpFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\r\n",
        "        \"\"\"\r\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\r\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\r\n",
        "        observation should be warped.\r\n",
        "        \"\"\"\r\n",
        "        super().__init__(env)\r\n",
        "        self._width = width\r\n",
        "        self._height = height\r\n",
        "        self._grayscale = grayscale\r\n",
        "        self._key = dict_space_key\r\n",
        "        if self._grayscale:\r\n",
        "            num_colors = 1\r\n",
        "        else:\r\n",
        "            num_colors = 3\r\n",
        "\r\n",
        "        new_space = gym.spaces.Box(\r\n",
        "            low=0,\r\n",
        "            high=255,\r\n",
        "            shape=(self._height, self._width, num_colors),\r\n",
        "            dtype=np.uint8,\r\n",
        "        )\r\n",
        "        if self._key is None:\r\n",
        "            original_space = self.observation_space\r\n",
        "            self.observation_space = new_space\r\n",
        "        else:\r\n",
        "            original_space = self.observation_space.spaces[self._key]\r\n",
        "            self.observation_space.spaces[self._key] = new_space\r\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\r\n",
        "\r\n",
        "    def observation(self, obs):\r\n",
        "        if self._key is None:\r\n",
        "            frame = obs\r\n",
        "        else:\r\n",
        "            frame = obs[self._key]\r\n",
        "\r\n",
        "        if self._grayscale:\r\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\r\n",
        "        frame = cv2.resize(\r\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\r\n",
        "        )\r\n",
        "        if self._grayscale:\r\n",
        "            frame = np.expand_dims(frame, -1)\r\n",
        "\r\n",
        "        if self._key is None:\r\n",
        "            obs = frame\r\n",
        "        else:\r\n",
        "            obs = obs.copy()\r\n",
        "            obs[self._key] = frame\r\n",
        "        return obs\r\n",
        "\r\n",
        "\r\n",
        "class FrameStack(gym.Wrapper):\r\n",
        "    def __init__(self, env, k):\r\n",
        "        \"\"\"Stack k last frames.\r\n",
        "        Returns lazy array, which is much more memory efficient.\r\n",
        "        See Also\r\n",
        "        --------\r\n",
        "        baselines.common.atari_wrappers.LazyFrames\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.k = k\r\n",
        "        self.frames = deque([], maxlen=k)\r\n",
        "        shp = env.observation_space.shape\r\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        ob = self.env.reset()\r\n",
        "        for _ in range(self.k):\r\n",
        "            self.frames.append(ob)\r\n",
        "        return self._get_ob()\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        ob, reward, done, info = self.env.step(action)\r\n",
        "        self.frames.append(ob)\r\n",
        "        return self._get_ob(), reward, done, info\r\n",
        "\r\n",
        "    def _get_ob(self):\r\n",
        "        assert len(self.frames) == self.k\r\n",
        "        return LazyFrames(list(self.frames))\r\n",
        "\r\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.ObservationWrapper.__init__(self, env)\r\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        # careful! This undoes the memory optimization, use\r\n",
        "        # with smaller replay buffers only.\r\n",
        "        return np.array(observation).astype(np.float32) / 255.0\r\n",
        "\r\n",
        "class LazyFrames(object):\r\n",
        "    def __init__(self, frames):\r\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\r\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\r\n",
        "        buffers.\r\n",
        "        This object should only be converted to numpy array before being passed to the model.\r\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\r\n",
        "        self._frames = frames\r\n",
        "        self._out = None\r\n",
        "\r\n",
        "    def _force(self):\r\n",
        "        if self._out is None:\r\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\r\n",
        "            self._frames = None\r\n",
        "        return self._out\r\n",
        "\r\n",
        "    def __array__(self, dtype=None):\r\n",
        "        out = self._force()\r\n",
        "        if dtype is not None:\r\n",
        "            out = out.astype(dtype)\r\n",
        "        return out\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self._force())\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        return self._force()[i]\r\n",
        "\r\n",
        "    def count(self):\r\n",
        "        frames = self._force()\r\n",
        "        return frames.shape[frames.ndim - 1]\r\n",
        "\r\n",
        "    def frame(self, i):\r\n",
        "        return self._force()[..., i]\r\n",
        "\r\n",
        "def make_atari(env_id, max_episode_steps=None):\r\n",
        "    env = gym.make(env_id)\r\n",
        "    assert 'NoFrameskip' in env.spec.id\r\n",
        "    env = NoopResetEnv(env, noop_max=30)\r\n",
        "    env = MaxAndSkipEnv(env, skip=4)\r\n",
        "    if max_episode_steps is not None:\r\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\r\n",
        "    return env\r\n",
        "\r\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\r\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\r\n",
        "    \"\"\"\r\n",
        "    if episode_life:\r\n",
        "        env = EpisodicLifeEnv(env)\r\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\r\n",
        "        env = FireResetEnv(env)\r\n",
        "    env = WarpFrame(env)\r\n",
        "    if scale:\r\n",
        "        env = ScaledFloatFrame(env)\r\n",
        "    if clip_rewards:\r\n",
        "        env = ClipRewardEnv(env)\r\n",
        "    if frame_stack:\r\n",
        "        env = FrameStack(env, 4)\r\n",
        "    return env"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4MJNY_Ky-x8"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1UIlQf3zJEV"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9lI4g6bzPhk",
        "outputId": "17261771-add9-4ee1-e5df-e0106d0e88c7"
      },
      "source": [
        "!apt-get install python-opengl -y\r\n",
        "\r\n",
        "!apt install xvfb -y\r\n",
        "\r\n",
        "!pip install pyvirtualdisplay\r\n",
        "\r\n",
        "!pip install piglet\r\n",
        "\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "Display().start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hCollecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f8fd6361e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec5phZ-UzSWE"
      },
      "source": [
        "import gym\r\n",
        "from IPython import display\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2y90pJGB-o5"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "# Configuration paramaters for the whole setup\r\n",
        "seed = 42\r\n",
        "gamma = 0.99  # Discount factor for past rewards\r\n",
        "epsilon = 1.0  # Epsilon greedy parameter\r\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\r\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\r\n",
        "epsilon_interval = (\r\n",
        "    epsilon_max - epsilon_min\r\n",
        ")  # Rate at which to reduce chance of random action being taken\r\n",
        "batch_size = 32  # Size of batch taken from replay buffer\r\n",
        "max_steps_per_episode = 10000\r\n",
        "\r\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\r\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\r\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\r\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\r\n",
        "#env.seed(seed)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Jv83aecfzX17",
        "outputId": "7b5b7f61-bfde-42b3-c677-f89e57fbb31e"
      },
      "source": [
        "env.reset()\r\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\r\n",
        "# for _ in range(40):\r\n",
        "#     img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "#     display.display(plt.gcf())\r\n",
        "#     display.clear_output(wait=True)\r\n",
        "#     action = env.action_space.sample()\r\n",
        "#     env.step(action)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hSWVkqGk0-k"
      },
      "source": [
        "num_actions = 4\r\n",
        "\r\n",
        "\r\n",
        "# def create_q_model():\r\n",
        "#     # Network defined by the Deepmind paper\r\n",
        "#     inputs = layers.Input(shape=(84, 84, 4,))\r\n",
        "\r\n",
        "#     # Convolutions on the frames on the screen\r\n",
        "#     layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\r\n",
        "#     layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\r\n",
        "#     layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\r\n",
        "\r\n",
        "#     layer4 = layers.Flatten()(layer3)\r\n",
        "\r\n",
        "#     layer5 = layers.Dense(512, activation=\"relu\")(layer4)\r\n",
        "#     action = layers.Dense(num_actions, activation=\"linear\")(layer5)\r\n",
        "\r\n",
        "#     return keras.Model(inputs=inputs, outputs=action)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYVtTyyTlWYX"
      },
      "source": [
        "def compute_conv_dim(dim_size, kernel_size, padding, stride):\r\n",
        "    return int((dim_size - kernel_size + 2 * padding) / stride + 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpCIADTAnhz2"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahg5RaP2lIPM"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "\r\n",
        "      def __init__(self,out = num_actions):\r\n",
        "              super(Net, self).__init__()\r\n",
        "\r\n",
        "              self.out = out # the action space based output\r\n",
        "\r\n",
        "              self.conv1 = nn.Conv2d(in_channels=4,out_channels=32,kernel_size = (8,8),stride = 4)\r\n",
        "              self.conv_out = compute_conv_dim(84, kernel_size=8, padding=0, stride=4)\r\n",
        "\r\n",
        "              self.conv2 = nn.Conv2d(32,64,kernel_size = (4,4),stride = 2)\r\n",
        "              self.conv_out = compute_conv_dim(self.conv_out, kernel_size=4, padding=0, stride=2)\r\n",
        "\r\n",
        "              self.conv3 = nn.Conv2d(64,64,kernel_size = (4,4),stride = 2)\r\n",
        "\r\n",
        "              self.conv_out = compute_conv_dim(self.conv_out, kernel_size=4, padding=0, stride=2)\r\n",
        "\r\n",
        "              self.l1_in_features = 64 * self.conv_out * self.conv_out\r\n",
        "\r\n",
        "              self.fc1 = nn.Linear(in_features=self.l1_in_features, out_features=512) \r\n",
        "              self.fc2 = nn.Linear(512,out_features = self.out) \r\n",
        "\r\n",
        "      def forward(self, x): \r\n",
        "\r\n",
        "              x = F.relu(self.conv1(x))\r\n",
        "              x = F.relu(self.conv2(x))\r\n",
        "              x = F.relu(self.conv3(x))\r\n",
        "              x = x.view(-1, self.l1_in_features)\r\n",
        "              x = F.relu(self.fc1(x))        \r\n",
        "              return self.fc2(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb31lngIoKVS"
      },
      "source": [
        "model_torch=Net()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJQh8jYLoPI3"
      },
      "source": [
        "state = np.array(env.reset())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3quEtk2ou8b"
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQM17JJpqkHg"
      },
      "source": [
        "x=np.moveaxis(state, -1, 0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Xliqw9qOjM"
      },
      "source": [
        "x=x.reshape(1,4,84,84)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SgLvrTooQfT",
        "outputId": "a73a7f74-1f75-4a23-8609-062249dd964d"
      },
      "source": [
        "X_batch = Variable(torch.from_numpy(x))\r\n",
        "\r\n",
        "X_batch=X_batch.float()\r\n",
        "model_torch(X_batch)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0310,  0.0174,  0.0376,  0.0044]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyTMyxTs2r9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c243c8-b99a-4100-e143-8ca757e517d9"
      },
      "source": [
        "# The first model makes the predictions for Q-values which are used to\r\n",
        "# make a action.\r\n",
        "model = Net()\r\n",
        "# Build a target model for the prediction of future rewards.\r\n",
        "# The weights of a target model get updated every 10000 steps thus when the\r\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\r\n",
        "model_target = Net()\r\n",
        "device=torch.device('cuda:0')\r\n",
        "model.to(device)\r\n",
        "model_target.to(device)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "  (fc1): Linear(in_features=576, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-BwpRAjgBQE"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.nn import functional"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1DQiJKG261-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "a42a0634-a395-4839-9ecf-45ad53af3311"
      },
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\r\n",
        "# improves training time\r\n",
        "optimizer = optim.Adam(model_target.parameters(), lr=0.00025,)\r\n",
        "loss_fn = nn.SmoothL1Loss()\r\n",
        "# Experience replay buffers\r\n",
        "action_history = []\r\n",
        "state_history = []\r\n",
        "state_next_history = []\r\n",
        "rewards_history = []\r\n",
        "done_history = []\r\n",
        "episode_reward_history = []\r\n",
        "running_reward = 0\r\n",
        "episode_count = 0\r\n",
        "frame_count = 0\r\n",
        "# Number of frames to take random action and observe output\r\n",
        "epsilon_random_frames = 50000\r\n",
        "# Number of frames for exploration\r\n",
        "epsilon_greedy_frames = 1000000.0\r\n",
        "# Maximum replay length\r\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\r\n",
        "max_memory_length = 100000\r\n",
        "# Train the model after 4 actions\r\n",
        "update_after_actions = 4\r\n",
        "# How often to update the target network\r\n",
        "update_target_network = 10000\r\n",
        "# Using huber loss for stability\r\n",
        "\r\n",
        "\r\n",
        "while True:  # Run until solved\r\n",
        "    env.reset()\r\n",
        "    state = np.array(env.reset())\r\n",
        "    \r\n",
        "    img = plt.imshow(env.render('rgb_array'))\r\n",
        "    state=np.moveaxis(state, -1, 0)\r\n",
        "    episode_reward = 0\r\n",
        "\r\n",
        "    for timestep in range(1, max_steps_per_episode):\r\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\r\n",
        "        display.display(plt.gcf())\r\n",
        "        display.clear_output(wait=True)\r\n",
        "\r\n",
        "        #Adding this line would show the attempts\r\n",
        "        # of the agent in a pop up window.\r\n",
        "        frame_count += 1\r\n",
        "        \r\n",
        "        # Use epsilon-greedy for exploration\r\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\r\n",
        "            # Take random action\r\n",
        "            action = np.random.choice(num_actions)\r\n",
        "        else:\r\n",
        "            # Predict action Q-values\r\n",
        "            # From environment state\r\n",
        "            with torch.no_grad():\r\n",
        "              X_batch = Variable(torch.from_numpy(state))\r\n",
        "              X_batch=X_batch.float()\r\n",
        "              X_batch=torch.unsqueeze(X_batch,dim=0).to(device)\r\n",
        "              action_probs = model(X_batch)\r\n",
        "              # Take best action\r\n",
        "              action = torch.argmax(action_probs[0]).cpu().numpy()\r\n",
        "\r\n",
        "        # Decay probability of taking random action\r\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\r\n",
        "        epsilon = max(epsilon, epsilon_min)\r\n",
        "\r\n",
        "        # Apply the sampled action in our environment\r\n",
        "        state_next, reward, done, _ = env.step(action)\r\n",
        "        state_next = np.array(state_next)\r\n",
        "        state_next=np.moveaxis(state_next, -1, 0)\r\n",
        "        reward = np.array(reward)\r\n",
        "        \r\n",
        "\r\n",
        "        episode_reward += reward\r\n",
        "\r\n",
        "        # Save actions and states in replay buffer\r\n",
        "        action_history.append(action)\r\n",
        "        state_history.append(state)\r\n",
        "\r\n",
        "        state_next_history.append(state_next)\r\n",
        "        done_history.append(done)\r\n",
        "        rewards_history.append(reward)\r\n",
        "        state = state_next\r\n",
        "\r\n",
        "        # Update every fourth frame and once batch size is over 32\r\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\r\n",
        "\r\n",
        "            # Get indices of samples for replay buffers\r\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\r\n",
        "          \r\n",
        "\r\n",
        "            # Using list comprehension to sample from replay buffer\r\n",
        "            state_sample = np.array([state_history[i] for i in indices])\r\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\r\n",
        "            rewards_sample = np.array([rewards_history[i] for i in indices])\r\n",
        "            action_sample = [action_history[i] for i in indices]\r\n",
        "            done_sample = torch.from_numpy(np.array([float(done_history[i]) for i in indices]))\r\n",
        "            \r\n",
        "\r\n",
        "            # Build the updated Q-values for the sampled future states\r\n",
        "            # Use the target model for stability\r\n",
        "            state_next_sample = Variable(torch.from_numpy(state_next_sample))\r\n",
        "            state_next_sample=state_next_sample.float()\r\n",
        "            state_next_sample=state_next_sample.to(device)\r\n",
        "            future_rewards = model_target(state_next_sample)\r\n",
        "            \r\n",
        "\r\n",
        "            # Q value = reward + discount factor * expected future reward\r\n",
        "            \r\n",
        "            updated_q_values = torch.from_numpy(rewards_sample).to(device) + gamma * torch.max(future_rewards,1)[0]\r\n",
        "            \r\n",
        "\r\n",
        "            # If final frame set the last value to -1\r\n",
        "            updated_q_values = updated_q_values * (1 - done_sample.to(device)) - done_sample.to(device)\r\n",
        "            \r\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\r\n",
        "            \r\n",
        "\r\n",
        "            masks =  functional.one_hot(torch.tensor(np.array(action_sample)), num_classes=num_actions)\r\n",
        "            \r\n",
        "\r\n",
        "        \r\n",
        "            # Train the model on the states and updated Q-values\r\n",
        "            \r\n",
        "           \r\n",
        "            state_sample = Variable(torch.from_numpy(state_sample))\r\n",
        "            state_sample=state_sample.float()\r\n",
        "            state_sample=state_sample.to(device)\r\n",
        "            q_values = model(state_sample)\r\n",
        "            \r\n",
        "          \r\n",
        "\r\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\r\n",
        "            q_action = torch.sum(torch.mul(q_values.to(device), masks.to(device)), axis=1)\r\n",
        "            # Calculate loss between new Q-value and old Q-value\r\n",
        "\r\n",
        "            # Backpropagation\r\n",
        "            loss = loss_fn(updated_q_values.double(), q_action.double())\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            for param in model.parameters():\r\n",
        "                param.grad.data.clamp_(-1, 1)\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        if frame_count % update_target_network == 0:\r\n",
        "            # update the the target network with new weights\r\n",
        "            #model_target.set_weights(model.get_weights())\r\n",
        "            model_target.load_state_dict(model.state_dict())\r\n",
        "            # Log details\r\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\r\n",
        "            print(template.format(running_reward, episode_count, frame_count))\r\n",
        "\r\n",
        "        # Limit the state and reward history\r\n",
        "        if len(rewards_history) > max_memory_length:\r\n",
        "            del rewards_history[:1]\r\n",
        "            del state_history[:1]\r\n",
        "            del state_next_history[:1]\r\n",
        "            del action_history[:1]\r\n",
        "            del done_history[:1]\r\n",
        "\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "\r\n",
        "    # Update running reward to check condition for solving\r\n",
        "    episode_reward_history.append(episode_reward)\r\n",
        "    if len(episode_reward_history) > 100:\r\n",
        "        del episode_reward_history[:1]\r\n",
        "    running_reward = np.mean(episode_reward_history)\r\n",
        "\r\n",
        "    episode_count += 1\r\n",
        "\r\n",
        "    if running_reward > 40:  # Condition to consider the task solved\r\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\r\n",
        "        break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-15f61389d2d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just update the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n\u001b[0;32m-> 2103\u001b[0;31m                             bbox_extra_artists=bbox_artists)\n\u001b[0m\u001b[1;32m   2104\u001b[0m                     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pad_inches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m             \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2005\u001b[0m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2006\u001b[0;31m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2007\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m             \u001b[0mbboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1174\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1176\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1174\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1176\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpi_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mtranslated\u001b[0;34m(self, tx, ty)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;34m\"\"\"Construct a `Bbox` by translating this one by *tx* and *ty*.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_points\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlZ6HDOo4x1n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eoQk3eW2_Tl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}